\chapter{Separation by Known Modulation}

\marginpar{This chapter is based on the works that has been submitted in 2014 and 2015 together with my collegues Stefan Bayer and Nils Werner~\cite{stoeter14, stoeter15icassp}. Both publications were based on my original ideas. Furthermore the proposed method were extended and evaluated in SiSEC 2016 as presented in the last part of this chapter.}

% start coverted from dafx
In most scenarios for source separation, as presented in Chapter~\cite{objectives-and-challenges}, it is common to assume that the spectral harmonics do only partially overlap.
This enabled unsupervised algorithms like non-negative matrix factorization (NMF) to approximate the mixture from a lower-rank decomposition in an unsupervised way.
Such systems are described in \cite{smaragdis03} and \cite{virtanen07}.
Additionally the popularity of the class of NMF algorithms can be explained by the intuitive way in which they work on time-frequency representations of the mixture signal.

The separation of sound sources from a single channel mixture is considered as an under-determined case which does not have a single solution. Knowing the way in which source signals are mixed together is crucial to the quality of separation systems. In the context of speech separation even unsupervised methods can lead to good results. This is due to the fact that mixtures of speech signals (like in a cocktail party environment) show a high degree of statistical independence. Mixtures of musical instruments, however, are highly correlated which is a desired aim of musical performances in general.

In the context of musical instrument source separation, many researchers have focused on including prior information about the sources in their algorithms \cite{ozerov12}.
The availability and detail of such a-priori information varies. Often systems learn spectral as well as temporal cues from training data or parts of the mixture where only one instrument is active.
One example of such informed source separation systems is described by Ewert and M\"uller \cite{ewert12}.
It incorporates the pitch and onset information encoded in a MIDI file to improve the separation result.
% end coverted from dafx

In the case of highly overlapped signals of unison instruments, the score information is not useful.
Therefore, in this chapter we evaluate the feasibility to increase the prior information to use the fundamental frequency estimate.

\section{Using Fundamental Frequency}
\label{sub:frequency_modulation}

% Vibrato rate around 5 Hertz. Similar to the tremor in parkinson deseases~\cite{fletcher01}.

Frequency modulation caused by vibrato is a very common playing style for string instruments but also for woodwind and brass instruments.
Vibrato is an effect that is well studied especially in musicology.
Performers tend to perform a vibrato in the same way when repeating a performance.
This can be exploited in source separation scenarios.
Typically, vibratos have modulation frequencies (rates) which vary between 4 and 8 Hz.
Additionally vibrato rates vary across different instruments.
In \cite{macleod06} the vibrato width (frequency deviation) was found to be significantly different between violinists and violists performers.

As with the amplitude modulated case NMF lacks the ability to model time varying frequencies since the $\mathbf{W}$ matrix is stationary.
Several extensions for NMF have been proposed to improve the decomposition quality.
\cite{hennequin11} proposes frequency dependent activations matrices, \cite{smaragdis08} has developed a system which can be described as shift invariant NMF.
Another approach is to model the spectral pattern changes by Markov chains \cite{nakano10}. All these approaches attempt to model the non-stationary effects within the decomposition model.
In this paper we propose a method that increases the stationarity of the signal in preprocessing step and then use the standard NMF for the decomposition. \\



\subsection{Time Warping}
% Content by Stefan Bayer
We make use of \emph{time-warping} which refers to a mapping of the linear time scale $t$ to a warped time scale $\tau$ via a mapping function $\tau=w(t)$.
To ensure a unique mapping, the mapping function needs to be strictly increasing.
For the discrete time case the mapping can be achieved by a time-varying re-sampling of the linear (i.e. regularly sampled) time signal under consideration.
The instantaneous sampling frequency then corresponds to the first derivative of
the mapping function. Although the mapping can be done from any time-span
$I$ on the linear time scale to any time span $J$ on the warped time scale, in
the discrete time case it is advantageous to have the same number of samples
in the linear and warped time domain. This ensures that the average sampling
frequency is the same in both domains. Such time-warping approaches have already
been proposed for different purposes such as transform-based audio coding
\cite{edler09}. As in these applications, we derive the mapping function from
the varying instantaneous fundamental frequency in such a manner that the variation of the frequency is
reduced or removed. To be more precise the actual information needed is not
the absolute instantaneous fundamental frequency but only its change over time.
The discrete time warp map $w[n]$ is then simply the scaled sum of the relative
frequencies $f[n]$:
\begin{equation}
w[n]=N \frac{\sum^n_{l=0}{f[l]}}{\sum^N_{k=0}{f[k]}}  \qquad 0\leq n<N,
\end{equation}
where $N$ being the number of samples of the signal under consideration.
From the requirements for the mapping function it follows that the relative
frequency $f[n]$ has to be positive at all instants and preferably should not
exhibit large jumps.
For the mapping from linear to warped time now the linear domain sample points
$s[\nu]$ for the regularly spaced samples $x[\nu]$ in the warped domain are
found by inverting $w[n]$. These sample points are then used to re-sample the linear time
domain samples $x[n]$ to the warped time domain samples $x[\nu]$, in our case
by employing an 128 times oversampled FIR low-pass filter. This processing leads to a sampling rate contour which is proportional to the pitch contour. Or in other words, a fixed number of samples are obtained in each period of the signal with the varying fundamental frequency. Mutatis mutandis the sample points $s[\nu]$ can be used for the re-sampling from warped time domain to linear time
domain. \\

In this paper the time-warping was done globally over the full lengths of the
signals under consideration. The globally time-warped sample sequence
was then used in the further processing steps. In Figure~\ref{fig:timewarptime} we show the results of the warping process in the time domain. \\

A similar approach using frequency modulation to separate a harmonic
source from a mixture was proposed in \cite{wang95}. Here the
individual lines are demodulated to the base band using a combined frequency
tracking/demodulation approach. The difference to our approach is that first
the absolute instantaneous frequency for every harmonic line has to be known
instead of a relative frequency that is common to all harmonic lines of a single
source. This relative frequency might be obtained easier than its
absolute value for a mixed signal. Secondly every harmonic line has to be individually
frequency demodulated while in our approach the full signal is frequency demodulated in one algorithmic step.\\

\section{Single Note Instruments Dataset} % (fold)
\label{sub:test_set}
% subsection test_set (end)

To build a test set we selected 10 instrumental items noted in Table~\ref{tab:testset}. The items have each been generated by rendering C4 notes in a state of the art software sampler. All test have a duration of about three seconds. Items were equalized in loudness by using an iterative calculation of the loudness algorithm of the time varying Zwicker model. The implementation \cite{genesis12} was used. The 10 instrument items then generated 45 unique mixtures of two instruments each. The processing was done in 44.1~kHz / 16 bit.

\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{ l l l}
  Instrument & Vibrato &  General MIDI \# \\
  \hline
  Violin & yes & 40 \\
  Viola & yes & 41 \\
  Violon Cello & yes & 42 \\
  Trumpet & no & 56 \\
  Trombone & no & 57\\
  Horn & no & 60  \\
  Bariton Sax & yes & 67 \\ % TODO
  Oboe & no & 68\\
  Clarinet & no & 71\\
  Flute & yes & 73\\
\end{tabular}
\end{center}
\caption{Instrument item test set}
\label{tab:testset}
\end{table}

\section{Pitch Variation Informed Separation} % (fold)
\label{ssub:pitch_variation_informed_source_separation}

With the ability to remove the frequency modulation from a signal we can then include this system in a source separation system to address the non-stationarity issues of NMF based approaches. Figure~\ref{fig:warpingdemo} shows how this system works on a harmonic FM signal mixture. Plots (a) and (b) show the two input signals which are linearly mixed (c). For each source the warp contour needs to be calculated. The mixture is then warped with pitch variation estimates of source 1  (d) and source 2 (e). The actual separation/filtering of the sources is then done by using NMF which is not shown here. To separate the components from the warped mixture we used NMF on a spectrogram computed with a very long DFT (about 0.5 s). NMF can work unsupervised by detecting the more tonal $\textbf{W}$ component by using a spectral flatness measure. The separated signals (f) and (g) then need to be warped back into the original time domain resulting in (h) and (i).

It is important to clarify that this approach would not be able to separate two modulating instruments playing in unison without having prior knowledge about the individual modulation functions. Although a pitch variation estimate might be difficult to achieve in a mixture our approach shows that such a system can make sense.


\begin{figure}[t]
\begin{tikzpicture}
    \node (inputmeta) [inner sep=0pt] {
        \begin{tikzpicture}
            \node (input) [inner sep=0pt] {\resizebox{0.25\columnwidth}{!}{\input{Chapters/05_Separation_Known/figures/input}}};
            \node [inner sep=0pt,below of=input,node distance=1.75cm, label={[yshift=-0.3cm]below:Input}] (pitchvariation) {\resizebox{0.25\columnwidth}{!}{\input{Chapters/05_Separation_Known/figures/pitchvariation}}};
        \end{tikzpicture}
    };
    \node [right of=input,node distance=0.38\columnwidth,draw,minimum width=0.25\columnwidth,minimum height=0.16\columnwidth,inner sep=0pt] (TW) {TW};
    \node [right of=TW,node distance=0.33\columnwidth,inner sep=0pt, label={[yshift=-0.76cm]below:Output}] (output) {\resizebox{0.25\columnwidth}{!}{\input{Chapters/05_Separation_Known/figures/output}}};
    \path[draw,->, line width=0.4mm] ($(inputmeta.east) + (0.0 ,+0.6)$) -- ($(TW.west) + (-0.08 ,+0.2)$);
    \path[draw,->, line width=0.4mm] ($(inputmeta.east) + (0.0 ,-0.6)$) -- ($(TW.west) + (-0.08 ,-0.2)$);
    \path[draw,->, line width=0.4mm] (TW) -- (output);
\end{tikzpicture}
\caption{Example of applying warping to an input signal by using a frequency variation contour.}
\label{fig:timewarptime}
\end{figure}

% from dafx
We wanted to evaluate the methods proposed in Sections~\ref{sub:am} and \ref{sub:frequency_modulation} so that they show the fundamental differences in their separation quality. Like in \cite{barker13} we choose not to address the problem of clustering the components after the matrix factorization operation. Instead of processing mixtures in a $A-B-AB$ or $A-AB-B$ paradigm we went for a supervised learning phase where we had access to the original source individually. In this \emph{oracle} supervised approach for each of the sources we then learned the spectral, temporal (for NMF), and modulation gain components (for MOD-NTF) and concatenated them. The learned coefficients were then used to initialize the final factorization process. This way we can achieve the maximum possible quality.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Algorithms} % (fold)
% \label{sub:algorithms}

% from dafx
The test set was processed by three algorithms: standard NMF, pitch variation informed NMF (PVI-NMF) (Section~\ref{ssub:pitch_variation_informed_source_separation}) and the non-negative tensor factorization based on modulation spectra (MOD-NTF) as described in Section~\ref{sub:am}). All factorizations for NMF and NTF were computed by minimizing the $\beta = 1$ divergence (Kullback-Leibler divergence). The Pitch Variation Informed-NMF (PVI-NMF) has been set up in the same way as the other algorithms. We choose to calculate results with $K=2$ and $K=4$. The pitch variation estimator is based on a method that was proposed by B\"ackstr\"om in 2009 \cite{backstrom09} with a subsequent post-processing to ensure the smoothness of the mapping. \\

Each of the algorithms did perform on the same filter bank and with the same sample rate. NMF approach did use a 2048 STFT with 512 samples hop size. For the MOD-NTF a second STFT based filter bank was used with 256 sample DFT size and 64 sample hop size. All methods use soft masking / wiener filtering for the actual synthesis.

\subsection{Evaluation}
% \label{sec:results}

The results were evaluated by using commonly used evaluation measures provided by the PEASS Toolbox \cite{emiya11}. The evaluation measure are:

\begin{itemize}
  \item Overall Perceptual Score (OPS)
  \item Target-related Perceptual Score (TPS)
  \item Interference-related Perceptual Score (IPS)
  \item Artifacts-related Perceptual Score (APS)
  \item Signal to Distortion Ratio (SDRi)
  \item Source to Interference Ratio (SIRi)
  \item Sources to Artifacts Ratio (SARi) \footnote{The $i$ indicates that these scores have been calculated by decomposition with PEASS \cite{emiya11} instead of \textsc{BSS Eval}.}
\end{itemize}

The mean values of the PEASS evaluation are provided in Table~\ref{tab:results}. It can be seen that the SDR values give a different tendency than the OPS score, showing that the differences between both measures are substantial. Since unison mixtures are even very challening for humans to segregate we chose to focus on the psycho-acoustically weighted performance measures only. The results show a slightly better overall performance for the PVI-NMF. A more fine grained overview from the OPS results experiment is presented in Figure~\ref{tab:resultsmatrix}. It can be seen that results vary a lot between the mixtures. The modulation tensor factorization (MOD-NTF) performs good on mixtures like Clarinet-Viola (71-41) or Clarinet-Cello (71-42)  where one source has vibrato and the other does not (see plots (e,f)). Although it performs well on average, MOD-NTF shows a high variance in the OPS results. The results have also been evaluated and confirmed subjectively by informal listening. Additionally we provide selected stimuli online on an acompanying webpage \footnote{\url{http://www.audiolabs-erlangen.de/resources/2014-DAFx-Unison/}}. In general the PEASS scores give a good indication of quality. However the artifacts that are introduced by the standard NMF synthesis seem to be not well reflected. One possible reason is that PEASS toolbox has not been tested on artifacts from unison mixtures. \\
Future work could include a robust multi pitch variation estimator for musical instruments. Salamon and Gomez \cite{salamon12} describe the current state of the art of f0 estimation. Some approaches use source separation to estimate multiple f0 pitch tracks. Therefore our approach shows that a robust multi pitch f0 estimate can also help to improve source separation. In the future an iterative multi-step procedure could lead to better results in both problem domains.

This paper proposes a new source separation scenario for instruments played in unison. It highlights the time-varying aspects of the signal sources like amplitude or frequency modulations. By addressing these aspects, the separation quality for non-unison mixtures can generally be improved, too.
Furthermore we present two methods to decompose those mixtures based on differences in the amplitude or frequency modulation of the sources. One is using a method already published based on a modulation tensor factorization. The other is a novel method that uses an estimate of the pitch variation of the two input sources to warp the mixture. Within the warped domain the frequency modulation of the desired source is removed so that the sources can be separated more easily from the mixture. The results of 45 mixtures have been evaluated by using the PEASS toolbox. The scores indicate an improvement of about 2 OPS points in favor of the pitch variation informed NMF compared to the standard NMF.

\begin{table}
\begin{center}
\small
\begin{tabular}{ r | r r r }
  Algorithm & NMF & PVI-NMF & MOD-NTF \\
  \hline
  OPS & 15.76 & \textbf{17.64} & 17.35 \\
  TPS & 30.17 & 32.80 & \textbf{34.03} \\
  IPS & 26.07 & \textbf{27.03} & 22.73 \\
  APS & 46.14 & \textbf{54.74} & 46.06 \\
  \hline
  SDRi & \textbf{2.96} & 2.54 & 2.20 \\
  SIRi & 2.31 & 1.80 & \textbf{3.13} \\
  SARi & 22.87 & 23.35 & \textbf{26.09} \\
\end{tabular}
\end{center}
  \caption{Results from Evaluation with PEASS 2.0 Toolbox \cite{emiya11}. Best performing algorithm is marked bold.}
  \label{tab:results}
\end{table}

\begin{figure}[H]
\begin{center}
\begin{tabular}{cc}
    \includegraphics[width=35mm]{Chapters/05_Separation_Known/figures/PEASSplots/OPS/NMF2.png} & \includegraphics[width=35mm]{Chapters/05_Separation_Known/figures/PEASSplots/OPS/NMF4.png} \\
    (a) NMF $K=2$ & (b) NMF $K=4$ \\[6pt]
        \includegraphics[width=35mm]{Chapters/05_Separation_Known/figures/PEASSplots/OPS/NMFWARP2.png} & \includegraphics[width=35mm]{Chapters/05_Separation_Known/figures/PEASSplots/OPS/NMFWARP4.png} \\
    (c) PVI-NMF $K=2$ & (d) PVI-NMF $K=4$ \\[6pt]
         \includegraphics[width=35mm]{Chapters/05_Separation_Known/figures/PEASSplots/OPS/NTF2.png} & \includegraphics[width=35mm]{Chapters/05_Separation_Known/figures/PEASSplots/OPS/NTF4.png} \\
    (e) MOD-NTF $K=2$ & (f) MOD-NTF $K=4$ \\[6pt]
\end{tabular}
\caption{Results of Overall Perceptual Score. Each matrix represents the mean OPS values for each individual mixture of two sources. The x and y axis represent the instrument IDs in General MIDI notation (See Table~\ref{tab:testset}).}
\label{tab:resultsmatrix}
\end{center}

\end{figure}

\begin{figure}[H]
    \centering
    \tiny
    \subfloat[Source 1]{
        \begin{tikzpicture}
            \begin{axis}[mystyle]
            \addplot [forget plot] graphics [xmin=0.5,xmax=1121.5,ymin=0.5,ymax=273.5] {Chapters/05_Separation_Known/figures/src1-1.png};
            \end{axis}
        \end{tikzpicture}%
    }\hfill
    \subfloat[Source 2]{
        \begin{tikzpicture}
            \begin{axis}[mystyle]
            \addplot [forget plot] graphics [xmin=0.5,xmax=1121.5,ymin=0.5,ymax=273.5] {Chapters/05_Separation_Known/figures/src2-1.png};
            \end{axis}
        \end{tikzpicture}%
    }

    \subfloat[Mixture]{
        \begin{tikzpicture}
            \begin{axis}[mystyle]
            \addplot [forget plot] graphics [xmin=0.5,xmax=1121.5,ymin=0.5,ymax=273.5] {Chapters/05_Separation_Known/figures/mix-1.png};
            \end{axis}
        \end{tikzpicture}%
    }

    \subfloat[Mix. warped by Pitch 1]{
        \begin{tikzpicture}
            \begin{axis}[mystyle]
            \addplot [forget plot] graphics [xmin=0.5,xmax=1121.5,ymin=0.5,ymax=273.5] {Chapters/05_Separation_Known/figures/mixwrpedsrc1-1.png};
            \end{axis}
        \end{tikzpicture}%
    }\hfill
    \subfloat[Mix. warped by Pitch 2]{
        \begin{tikzpicture}
            \begin{axis}[mystyle]
            \addplot [forget plot] graphics [xmin=0.5,xmax=1121.5,ymin=0.5,ymax=273.5] {Chapters/05_Separation_Known/figures/mixwrpedsrc2-1.png};
            \end{axis}
        \end{tikzpicture}%
    }

    \subfloat[Target 1 warped]{
        \begin{tikzpicture}
            \begin{axis}[mystyle]
            \addplot [forget plot] graphics [xmin=0.5,xmax=1121.5,ymin=0.5,ymax=273.5] {Chapters/05_Separation_Known/figures/src1wrped-1.png};
            \end{axis}
        \end{tikzpicture}%
    }\hfill
    \subfloat[Target 2 warped]{
        \begin{tikzpicture}
            \begin{axis}[mystyle]
            \addplot [forget plot] graphics [xmin=0.5,xmax=1121.5,ymin=0.5,ymax=273.5] {Chapters/05_Separation_Known/figures/src2wrped-1.png};
            \end{axis}
        \end{tikzpicture}%
    }

    \subfloat[Target 1 unwarped]{
        \begin{tikzpicture}
            \begin{axis}[mystyle]
            \addplot [forget plot] graphics [xmin=0.5,xmax=1121.5,ymin=0.5,ymax=273.5] {Chapters/05_Separation_Known/figures/src1unwarped-1.png};
            \end{axis}
        \end{tikzpicture}%
    }\hfill
    \subfloat[Target 2 unwarped]{
        \begin{tikzpicture}
            \begin{axis}[mystyle]
            \addplot [forget plot] graphics [xmin=0.5,xmax=1121.5,ymin=0.5,ymax=273.5] {Chapters/05_Separation_Known/figures/src2unwarped-1.png};
            \end{axis}
        \end{tikzpicture}%
    }
    \caption{Example of pitch variation informed NMF in the warped domain. \textit{Time} is shown on horizontal axes. \textit{Frequency} is shown on vertical axes.}
    \label{fig:warpingdemo}
\end{figure}

\section{Applying Pitch Variation Informed Separation to Music}

In the previous section we showed the effectiveness of a pitch variation informed separation system.
In this section I now want to explain how such a system can be extended to be used for real world application such as the separation of music into lead/vocals and the accompaniment track.

One obvious drawback of our method is that it relies on a robust pitch variation estimator which is usually difficult to obtain from a mixtures where multiple harmonic sources are present at the same time.

In a first step, the ``Melodia'' algorithm~\cite{salamon12} is used to obtain an estimate of the predominant melody from the mixture. The mixture is then time warped based on the fundamental frequency of the melody so that itâ€™s predominant solo part is nearly constant in F0. (See~\cite{salamon12} for details). The extraction is then carried out in time domain using efficient comb filtering.
The algorithm is implemented in Python and has low complexity resulting in approximately 0.33s for 1s of audio on 3,1 GHz Intel Core i5.

* Mention that we want to extend the methods based in~\cite{stoeter14} and take it to SiSEC evaluation challange.
* Therefore we need a) a robust f0 estimation b) a robust voice activity detector and c) a good filtering paradigm.

\subsection{Results on SiSEC 2015}
\label{ssec:performance}

\subsection{Source Extraction}


\subsection{Improving Voiced/Unvoiced Detection}

\begin{figure}
  \input{Chapters/05_Separation_Known/figures/sisec16}
  \caption{BSS Eval scores for the vocals and accompaniment estimates on the DSD100 dataset as used in~\cite{liutkus17}. Results are shown on the \emph{test} set only. Results indicate the improvements of the DNN based vocal activity detection system.}
  \label{fig:05_comparison_sto_stodnn}
\end{figure}

\section{Excursus: Improving $F0$ Estimation using warped method} % (fold)
\label{sec:method}

Normally an informed system could be build by using some informed system that uses an upper bound.
However the upper bound for a warping based system is hard because we are essentially not just building an F0 system.

\subsection{$F0$ Estimation Methods}

An estimate of the fundamental frequency $F0$ of a signal is required in various applications of audio and speech signal processing. $F0$ is often synonymously referred to as pitch which is a perceptual measure. In the past, a number of algorithms were presented to provide such estimates, with many of them being designed for specific applications. Some scenarios are targeted to extract the fundamental frequency of the predominant source~\cite{salamon12} in a mixture of other sources. In other applications, algorithms are used to extract fundamental frequencies of multiple sources simultaneously present in a signal~\cite{klapuri2003multiple}. However, the most common scenario in many works is to extract the fundamental frequency of a monophonic and harmonic audio signal containing speech or music~\cite{talkin1995robust, boersma2002praat, de2002yin, resch, camacho2007swipe, tidhar2010high, christensen2007joint}.
%
The development of novel methods for fundamental frequency estimation, performing as well as earlier methods, such as the popular correlation based \textsc{YIN} algorithm~\cite{de2002yin}, has proven challenging. In a recent study~\cite{babacan2013comparative} it is stated that YIN still clearly performs best in terms of accuracy. Nevertheless, when using YIN or other block based algorithms, a frame length and a hop size have to be selected trading temporal resolution on one side against frequency accuracy and robustness on the other side.

Especially when the signal is polyphonic, the robustness is the most crucial aspect of a pitch estimator. In recent work from Mauch et al.~\cite{mauch2014pyin}, the robustness of the \textsc{YIN} algorithm is improved by probabilistic post-processing. However, besides robustness, there is a variety of use cases requiring high accuracy as well as high temporal resolution. Application in parametric audio coding~\cite{purnhagen2000hiln} requires the parameterization of pitch bends and vibratos. Furthermore, source separation algorithms aiming at the extraction of harmonic sources from the mixture can make use of an instantaneous $F0$ estimate~\cite{virtanen2008combining, stoterunison}. There are already contributions addressing the improvement of accuracy of $F0$ estimates such as~\cite{medan1991super} which introduced a non-integer similarity model or~\cite{christensen2007joint} which belongs to the group of parametric pitch estimators.

We propose to improve the output of already existing algorithms in terms of temporal resolution as well as accuracy by iterative time warping. Two other contributions already make use of time warping in the context of pitch estimation. Resch et al.~\cite{resch} proposed an instantaneous pitch estimation technique which optimizes a warping function that would lead to a constant pitch signal. Their optimization framework minimizes a cost function specifically targeted for speech signals. Azarov et al.\ have introduced an improved version of RAPT (called iRAPT1 and iRAPT2) which also uses time warping to some extent~\cite{azarov2012instantaneous} but misses an additional step as will be shown in Section~?.
Our main contribution is a time warping based refinement method that is applicable to any F0 estimate. Our method emphasizes the strengths of different estimators and thus can even help to improve their robustness. In the following, we will describe the refinement method (Section~?) and show the experimental evaluation and its results (Section~?).

Depending on the algorithm and application, there are several reasons why $F0$ estimators deliver a less than ideal performance. When the signal tested is not tonal --- like in unvoiced parts of speech --- a proper estimation is impossible. If the estimator is optimized on purely harmonic signals, inharmonicity or frequency jitter of the input signal will increase the estimation error. Many of these reasons will lead to errors on the coarse level of the estimate (like octave jumps). The fine level accuracy is mostly influenced by parameters like time and/or frequency resolution of the estimator. A signal containing rapid changes of the frequency or modulations like ``vibrato'' is therefore more affected regarding fine level error. To obtain a more accurate estimate, we propose to time warp the signal by using the coarse level estimate towards a more constant pitch. The underlying assumption here is that pitch estimators generally perform better the more constant the pitch is.
In this section, we formulate the mathematical background of the time warping and present our proposed method for obtaining a refined $F0$ estimate.
% \subsection{Initial $F0$ estimate}
% \label{sub:initial_estimate}

The first step is to calculate an initial $F0$ estimate by using an existing pitch estimator. Note that we later require the estimate to be defined for every input sample, thus $\Pitch[n]$ may require interpolation. In our pipeline, we use linear interpolation for all estimators. $F0$ estimators, like YIN~\cite{de2002yin}, also provide a measure of confidence $c[n]$.
\subsection{Time warping}
\label{sub:Refinement}

In this step, we apply \emph{time warping} which refers to a strictly monotonous mapping
of the natural or linear time scale $t$ to a warped time scale $\tau$ via a
mapping function $\tau=w(t)$.
The mapping between the two domains for the continuous time case then is:
\begin{equation}\label{eq:contWarpedTime}
\breve{x}(\tau)=x(w^{-1}(\tau)), \quad x(t)=\breve{x}(w(t))
\end{equation}
where $x(t)$ is the linear-time signal and $\breve{x}(\tau)$ is the warped-time signal.
For the discrete time case, the signals in both linear-time and warped-time domains are sampled
using a constant sample interval $T$. With sample indices $\nu$ and $n$ for the warped-time domain and linear time-domain respectively, the warping is performed by

\begin{align}
\breve{x}[\nu] &= x(\sigma[\nu]) & \textrm{ with } & \sigma[\nu] = w^{-1}(\nu T), \\
\intertext{and the inverse warping by}
x{}[n] &= \breve{x}(s[n]) & \textrm{ with } & s{}[n] = w(nT).
\end{align}

\subsubsection{Warp contour}
\label{subs:warp_contour}

In our application, the warp map $w(t)$ is constructed in such a way that the instantaneous changes in frequency of the signal in the linear time domain are minimized in the warped time domain. For this, we derive the map from an estimate of the fundamental frequency $F0$.

For processing, the actual information needed is not the absolute instantaneous fundamental frequency but only its change over time. This means that the warping contour can be derived from an algorithm which may differ from the actual $F0$ estimator.

The discrete time warp map $w[n]$ is the scaled sum of the relative
frequency contour (the \emph{warp contour}) $W[n]$:
\begin{equation}
w[n]=N \frac{\sum^n_{l=0}{W[l]}}{\sum^{N-1}_{k=0}{W[k]}}  \qquad 0\leq n<N,
\end{equation}
where $N$ being the number of samples of the signal under consideration.
As stated above the full warp map $w(t)$ is then obtained by linearly interpolating $w[n]$. From the requirements for the mapping function it follows that $W[n]$ has to be greater than zero for all $n$. In the case of a perfect $F0$ estimate, the signal warped with the resulting contour would have a constant $F0$ equal to the average $\bar{W}$.

In the scope of this work, the warping is applied globally over the full length of the signals under consideration. An optional confidence measure $c[n]$ can be incorporated for a processed version of the warping contour. This ensures that the warp contour has no discontinuities that result in additional artifacts after re-sampling. If the estimator does not provide such a measure, a separate voiced/unvoiced detection algorithm can be used. To obtain a warp contour $W[n]$ from an $F0$ estimate we propose the following steps: \textbf{(A)} initialize the warp contour with $F0$ estimate $W = \Pitch$, \textbf{(B)} find contour segments with high confidence, i.e. $c[n]$ exceeds a given threshold, \textbf{(C)} linearly connect the high confidence contour segments and \textbf{(D)} set start and end of warp contour to a constant value if confidence is below threshold. That way warping according to $F0$ is applied in the regions of high confidence without significantly affecting the gaps in-between.

\subsection{Evaluation}
