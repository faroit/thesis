\chapter{Separation by Known Modulation}
\label{cha:known}

% start coverted from dafx
In most scenarios for source separation, as presented in Chapter~\ref{objectives-and-challenges}, it is common to assume that the spectral harmonics do only partially overlap.
This enabled unsupervised algorithms like non-negative matrix factorization (NMF) to approximate the mixture from a lower-rank decomposition in an unsupervised way.
Such systems are described in \cite{smaragdis03} and \cite{virtanen07} and further details will be given later in Chapter~\ref{cha:unknown}.
Additionally the popularity of the class of NMF algorithms can be explained by the intuitive way in which they work on time-frequency representations of the mixture signal.
\par
The separation of sound sources from a single channel mixture is considered as an under-determined case which does not have a single solution. Knowing the way in which source signals are mixed together is crucial to the quality of separation systems. In the context of speech separation even unsupervised methods can lead to good results. This is due to the fact that mixtures of speech signals (like in a cocktail party environment) show a high degree of statistical independence. Mixtures of musical instruments, however, are highly correlated which is a desired aim of musical performances in general.

In the context of musical instrument source separation, many researchers have focused on including prior information about the sources in their algorithms \cite{ozerov12}.
The availability and detail of such a-priori information varies. Often systems learn spectral as well as temporal cues from training data or parts of the mixture where only one instrument is active.
One example of such informed source separation systems is described by Ewert and M\"uller \cite{ewert12}.
It incorporates the pitch and onset information encoded in a MIDI file to improve the separation result.
% end coverted from dafx

In the case of highly overlapped signals of unison instruments, the score information is not useful.
Therefore, in this work, we evaluate the feasibility to increase the prior information to use the fundamental frequency estimate of the source to be extracted.

From this starting point, two kinds of approaches can be distinguished between, depending on how they exploit this information.
The first option to obtain the separated source is to re-synthesize it using a sinusoidal model, which has been studied in several methods which are not detailed in this thesis (for a detailed overview, I refer the reader to Section III A of~\cite{rafii}).

Due to the problem that sinusoidal synthesis suffers from a typical \textit{metallic} sound quality, which is mostly due to discrepancies between the estimated excitation signals of the lead signal compared to the ground truth, we decided to take an alternative approach which is to exploit harmonicity in another way, by filtering out everything from the mixture that is not located close to the detected harmonics.
\par
In the past many research has been focused on such a procedure as it turned out to be a common task in source separation systems:

E.g. Li and Wang proposed to use a vocal/non-vocal classifier and a predominant pitch detection algorithm \cite{li06, li07}. They first detected the singing voice by using a spectral change detector \cite{duxbury03} to partition the mixture into homogeneous portions, and GMMs on MFCCs to classify the portions as vocal or non-vocal. Then, they used the predominant pitch detection algorithm in~\cite{li05} to detect the pitch contours from the vocal portions, extending the multi-pitch tracking algorithm in~\cite{wu03}. Finally, they extracted the singing voice by decomposing the vocal portions into TF units and labeling them as singing or accompaniment dominant, extending the speech separation algorithm in \cite{hu02}.

Han and Raphael proposed an approach for desoloing a recording of a soloist with an accompaniment given a musical score and its time alignment with the recording \cite{han07}. They derived a mask \cite{roweis01} to remove the solo part after using an EM algorithm to estimate its melody, that exploits the score as side information.

Hsu et al. proposed an approach which also identifies and separates the unvoiced singing voice \cite{hsu08,hsu10}. Instead of processing in the STFT domain, they use the perceptually motivated gammatone filter-bank as in~\cite{hu02,li07}. They first detected accompaniment, unvoiced, and voiced segments using an HMM and identified voice-dominant TF units in the voiced frames by using the singing voice separation method in \cite{li07}, using the predominant pitch detection algorithm in \cite{dressler062}. Unvoiced-dominant TF units were identified using a GMM classifier with MFCC features learned from training data. Finally, filtering was achieved with spectral subtraction~\cite{scalart96}.

Raphael and Han then proposed a classifier-based approach to separate a soloist from accompanying instruments using a time-aligned symbolic musical score \cite{raphael08}. They built a tree-structured classifier \cite{breiman84} learned from labeled training data to classify TF points in the STFT as belonging to solo or accompaniment. They additionally constrained their classifier to estimate masks having a connected structure.

Cano et al. proposed various approaches for solo and accompaniment separation. In~\cite{cano09}, they separated saxophone melodies from mixtures with piano and/or orchestra by using a melody line detection algorithm, incorporating information about typical saxophone melody lines. In~\cite{grollmisch11,dittmar12,cano12}, they proposed to use the pitch detection algorithm in~\cite{dressler11}. Then, they refined the fundamental frequency and the harmonics, and created a binary mask for the solo and accompaniment. They finally used a post-processing stage to refine the separation. In \cite{cano13}, they included a noise spectrum in the harmonic refinement stage to also capture noise-like sounds in vocals. In \cite{cano14}, they additionally included common amplitude modulation characteristics in the separation scheme.

Bosch et al. proposed to separate the lead instrument using a musical score \cite{bosch12}. After a preliminary alignment of the score to the mixture, they estimated a score confidence measure to deal with local misalignments and used it to guide the predominant pitch tracking. Finally, they performed low-latency separation based on the method in \cite{marxer12}, by combining harmonic masks derived from the estimated pitch and additionally exploiting stereo information as presented later in Section~\ref{sec:multichannel}.

Vaneph et al. proposed a framework for vocal isolation to help spectral editing \cite{vaneph16}. They first used a voice activity detection process based on a deep learning technique \cite{leglaive15}. Then, they used pitch tracking to detect the melodic line of the vocal and used it to separate the vocal and background, allowing a user to provide manual annotations when necessary.
\par
In the following, I present parts of work that has been published in 2014~\cite{stoeter14}.

\section{Pitch variation informed Separation}
\label{sub:frequency_modulation}

\marginpar{This section is based on the work that has been submitted in 2014 and together with my collegues Stefan Bayer ~\cite{stoeter14}. Both publications were based on my original ideas}

% TODO add signal model here
% \begin{equation}
%   \mathbf{x}=\sum_{j=1}^{J}a_j\mathbf{s}_j
% \end{equation}

Frequency modulation caused by vibrato is a very common playing style for string instruments but also for woodwind and brass instruments.
Vibrato is an effect that is well studied especially in musicology.
Performers tend to perform a vibrato in the same way when repeating a performance.
This can be exploited in source separation scenarios.
Typically, vibratos have modulation frequencies (rates) which vary between 4 and 8 Hz, which, interestingly, is similar to the tremor in parkinson deseases~\cite{fletcher01}.
Additionally, vibrato rates vary across different instruments.
In \cite{macleod06} the vibrato width (frequency deviation) was found to be significantly different between violinists and violists performers.

In the case of a standard NMF, it lacks the ability to model time varying frequencies since the $\mathbf{W}$ matrix is stationary.
Several extensions for NMF have been proposed to improve the decomposition quality.
\cite{hennequin11} proposes frequency dependent activations matrices, \cite{smaragdis08} has developed a system which can be described as shift invariant NMF.
Another approach is to model the spectral pattern changes by Markov chains \cite{nakano10}. All these approaches attempt to model the non-stationary effects within the decomposition model.
In this work, instead, we propose a method that increases the stationarity of the signal in preprocessing step and then use the standard separation like NMF for the decomposition.\\

\subsection{Time Warping}
\label{sub:time_warping}
% Content by Stefan Bayer
The idea is to make use of \emph{time-warping} which refers to a mapping of the linear time scale $t$ to a warped time scale $\tau$ via a mapping function $\tau=w(t)$.
To ensure a unique mapping, the mapping function needs to be strictly increasing.
For the discrete time case the mapping can be achieved by a time-varying re-sampling of the linear (i.e. regularly sampled) time signal under consideration.
The instantaneous sampling frequency then corresponds to the first derivative of
the mapping function. Although the mapping can be done from any time-span
$I$ on the linear time scale to any time span $J$ on the warped time scale, in
the discrete time case it is advantageous to have the same number of samples
in the linear and warped time domain. This ensures that the average sampling
frequency is the same in both domains. Such time-warping approaches have already
been proposed for different purposes such as transform-based audio coding
\cite{edler09}. As in these applications, we derive the mapping function from
the varying instantaneous fundamental frequency in such a manner that the variation of the frequency is
reduced or removed. To be more precise the actual information needed is not
the absolute instantaneous fundamental frequency but only its change over time.
The discrete time warp map $w[n]$ is then simply the scaled sum of the relative
frequencies $f[n]$:
\begin{equation}
w[n]=N \frac{\sum^n_{l=0}{f[l]}}{\sum^N_{k=0}{f[k]}}  \qquad 0\leq n<N,
\end{equation}
where $N$ being the number of samples of the signal under consideration.
From the requirements for the mapping function it follows that the relative
frequency $f[n]$ has to be positive at all instants and preferably should not
exhibit large jumps.
For the mapping from linear to warped time now the linear domain sample points
$s[\nu]$ for the regularly spaced samples $x[\nu]$ in the warped domain are
found by inverting $w[n]$. These sample points are then used to re-sample the linear time
domain samples $x[n]$ to the warped time domain samples $x[\nu]$, in our case
by employing an 128 times oversampled FIR low-pass filter. This processing leads to a sampling rate contour which is proportional to the pitch contour. Or in other words, a fixed number of samples are obtained in each period of the signal with the varying fundamental frequency. Mutatis mutandis the sample points $s[\nu]$ can be used for the re-sampling from warped time domain to linear time domain. \\

In this work, the time-warping was done globally over the full lengths of the
signals under consideration. The globally time-warped sample sequence
was then used in the further processing steps. In Figure~\ref{fig:timewarptime} we show the results of the warping process in the time domain. \\

A similar approach using frequency modulation to separate a harmonic
source from a mixture was proposed in \cite{wang95}. Here the
individual lines are demodulated to the base band using a combined frequency
tracking/demodulation approach. The difference to our approach is that first
the absolute instantaneous frequency for every harmonic line has to be known
instead of a relative frequency that is common to all harmonic lines of a single
source. This relative frequency might be obtained easier than its
absolute value for a mixed signal. Secondly every harmonic line has to be individually frequency demodulated while in our approach the full signal is frequency demodulated in one algorithmic step.\\

% subsection test_set (end)

\subsection{Separation} % (fold)
\label{sub:pitch_variation_informed_source_separation}

With the ability to remove the frequency modulation from a signal we can then include this system in a source separation system to address the non-stationarity issues of NMF based approaches. Figure~\ref{fig:warpingdemo} shows how this system works on a purly harmonic FM signal mixture. Plots (a) and (b) show the two input signals which are linearly mixed (c). For each source the warp contour needs to be calculated. The mixture is then warped with pitch variation estimates of source 1  (d) and source 2 (e). The actual separation/filtering of the sources is then done by using NMF which is not shown here. To separate the components from the warped mixture we used NMF on a spectrogram computed with a very long DFT (about 0.5 s). NMF can work unsupervised by detecting the more tonal $\textbf{W}$ component by using a spectral flatness measure. The separated signals (f) and (g) then need to be warped back into the original time domain resulting in (h) and (i).

It is important to clarify that this approach would not be able to separate two modulating instruments playing in unison without having prior knowledge about the individual modulation functions. Although a pitch variation estimate might be difficult to achieve in a mixture, our approach shows that such a system could work if that estimate is accurate.

\begin{figure}[t]
\begin{tikzpicture}
    \node (inputmeta) [inner sep=0pt] {
        \begin{tikzpicture}
            \node (input) [inner sep=0pt] {\resizebox{0.25\columnwidth}{!}{\input{Chapters/05_Separation_Known/figures/input}}};
            \node [inner sep=0pt,below of=input,node distance=1.75cm, label={[yshift=-0.3cm]below:Input}] (pitchvariation) {\resizebox{0.25\columnwidth}{!}{\input{Chapters/05_Separation_Known/figures/pitchvariation}}};
        \end{tikzpicture}
    };
    \node [right of=input,node distance=0.38\columnwidth,draw,minimum width=0.25\columnwidth,minimum height=0.16\columnwidth,inner sep=0pt] (TW) {TW};
    \node [right of=TW,node distance=0.33\columnwidth,inner sep=0pt, label={[yshift=-0.76cm]below:Output}] (output) {\resizebox{0.25\columnwidth}{!}{\input{Chapters/05_Separation_Known/figures/output}}};
    \path[draw,->, line width=0.4mm] ($(inputmeta.east) + (0.0 ,+0.6)$) -- ($(TW.west) + (-0.08 ,+0.2)$);
    \path[draw,->, line width=0.4mm] ($(inputmeta.east) + (0.0 ,-0.6)$) -- ($(TW.west) + (-0.08 ,-0.2)$);
    \path[draw,->, line width=0.4mm] (TW) -- (output);
\end{tikzpicture}
\caption{Example of applying warping to an input signal by using a frequency variation contour.}
\label{fig:timewarptime}
\end{figure}

\subsection{Evaluation}
We use the test set as described in Chapter~4 and selected 10 instrumental items noted in Table~\ref{tab:testset}. The items have each been generated by rendering C4 notes in a state of the art software sampler. All test have a duration of about three seconds. Items were equalized in loudness by using an iterative calculation of the loudness algorithm of the time varying Zwicker model. The implementation \cite{genesis12} was used. The 10 instrument items then generated 45 unique mixtures of two instruments each. The processing was done in 44.1~kHz / 16 bit.

\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{ l l l}
  Instrument & Vibrato &  General MIDI \# \\
  \hline
  Violin & yes & 40 \\
  Viola & yes & 41 \\
  Violon Cello & yes & 42 \\
  Trumpet & no & 56 \\
  Trombone & no & 57\\
  Horn & no & 60  \\
  Bariton Sax & yes & 67 \\ % TODO
  Oboe & no & 68\\
  Clarinet & no & 71\\
  Flute & yes & 73\\
\end{tabular}
\end{center}
\caption{Instrument item test set}
\label{tab:testset}
\end{table}

We evaluated the method in terms of separation quality.
Like in \cite{barker13} we choose not to address the problem of clustering the components after the matrix factorization operation.
Instead of processing mixtures in a $A-B-AB$ or $A-AB-B$ paradigm we went for a supervised learning phase where we had access to the original source individually.
In this \emph{oracle} supervised approach for each of the sources we then learned the spectral, temporal components and concatenated them. The learned coefficients were then used to initialize the final factorization process. This way we can achieve the upper bound separation result.

The test set was processed by two algorithms: standard NMF and the proposed pitch variation informed NMF (PVI-NMF). The factorizations for NMF and NTF were computed by minimizing the $\beta = 1$ divergence (Kullback-Leibler divergence).
We choose to calculate results with $K=2$ and $K=4$.
The pitch variation estimator is based on a method that was proposed by B\"ackstr\"om in 2009~\cite{backstrom09} with a subsequent post-processing to ensure the smoothness of the mapping. \\

Both algorithms did perform on the same filter bank output and with the same sample rate. The NMF approach did use a 2048 STFT with 512 samples hop size.
All methods use soft masking / wiener filtering for the actual synthesis.

The results were evaluated by using commonly used evaluation measures provided by the {PEASS} Toolbox~\cite{emiya11}. The evaluation measure are:

\begin{itemize}
  \item Overall Perceptual Score (OPS)
  \item Target-related Perceptual Score (TPS)
  \item Interference-related Perceptual Score (IPS)
  \item Artifacts-related Perceptual Score (APS)
  \item Signal to Distortion Ratio (SDRi)
  \item Source to Interference Ratio (SIRi)
  \item Sources to Artifacts Ratio (SARi) \footnote{The $i$ indicates that these scores have been calculated by decomposition with PEASS \cite{emiya11} instead of \textsc{BSS Eval}.}
\end{itemize}

The mean values of the PEASS evaluation are provided in Table~\ref{tab:results}. It can be seen that the SDR values give a different tendency than the OPS score, showing that the differences between both measures are substantial. Since unison mixtures are even very challenging for humans to segregate we chose to focus on the psycho-acoustically weighted performance measures only. The results show a slightly better overall performance for the PVI-NMF. A more fine grained overview from the OPS results experiment is presented in Figure~\ref{tab:resultsmatrix}.
The results have also been evaluated and confirmed subjectively by informal listening. Additionally we provide selected stimuli online on an accompanying webpage \footnote{\url{http://www.audiolabs-erlangen.de/resources/2014-DAFx-Unison/}}. In general the PEASS scores give a good indication of quality. However the artifacts that are introduced by the standard NMF synthesis seem to be not well reflected. One possible reason is that PEASS toolbox has not been tested on artifacts from unison mixtures. \\

Our paper proposes a new source separation scenario for instruments played in unison. It highlights the time-varying aspects of the signal sources like amplitude or frequency modulations. By addressing these aspects, the separation quality for non-unison mixtures can generally be improved, too.
Furthermore we present two methods to decompose those mixtures based on differences in the amplitude or frequency modulation of the sources. One is using a method already published based on a modulation tensor factorization. The other is a novel method that uses an estimate of the pitch variation of the two input sources to warp the mixture. Within the warped domain the frequency modulation of the desired source is removed so that the sources can be separated more easily from the mixture. The results of 45 mixtures have been evaluated by using the PEASS toolbox. The scores indicate an improvement of about 2 OPS points in favor of the pitch variation informed NMF compared to the standard NMF.

\begin{table}
\begin{center}
\small
\begin{tabular}{ r | r r }
  Algorithm & NMF & PVI-NMF \\
  \hline
  OPS & 15.76 & \textbf{17.64}\\
  TPS & 30.17 & \textbf{32.80}\\
  IPS & 26.07 & \textbf{27.03}\\
  APS & 46.14 & \textbf{54.74}\\
  \hline
  SDRi & \textbf{2.96} & 2.54 \\
  SIRi & \textbf{2.31} & 1.80 \\
  SARi & 22.87 & \textbf{23.35} \\
\end{tabular}
\end{center}
  \caption{Results from Evaluation with PEASS 2.0 Toolbox \cite{emiya11}. Best performing algorithm is marked bold.}
  \label{tab:results}
\end{table}

% \begin{figure}[H]
% \begin{center}
% \begin{tabular}{cc}
%     \includegraphics[width=35mm]{Chapters/05_Separation_Known/figures/PEASSplots/OPS/NMF2.png} & \includegraphics[width=35mm]{Chapters/05_Separation_Known/figures/PEASSplots/OPS/NMF4.png} \\
%     (a) NMF $K=2$ & (b) NMF $K=4$ \\[6pt]
%         \includegraphics[width=35mm]{Chapters/05_Separation_Known/figures/PEASSplots/OPS/NMFWARP2.png} & \includegraphics[width=35mm]{Chapters/05_Separation_Known/figures/PEASSplots/OPS/NMFWARP4.png} \\
%     (c) PVI-NMF $K=2$ & (d) PVI-NMF $K=4$ \\[6pt]
%          \includegraphics[width=35mm]{Chapters/05_Separation_Known/figures/PEASSplots/OPS/NTF2.png} & \includegraphics[width=35mm]{Chapters/05_Separation_Known/figures/PEASSplots/OPS/NTF4.png} \\
%     (e) MOD-NTF $K=2$ & (f) MOD-NTF $K=4$ \\[6pt]
% \end{tabular}
% \caption{Results of Overall Perceptual Score. Each matrix represents the mean OPS values for each individual mixture of two sources. The x and y axis represent the instrument IDs in General MIDI notation (See Table~\ref{tab:testset}).}
% \label{tab:resultsmatrix}
% \end{center}

% \end{figure}

% TODO
% \begin{figure}[H]
%     \centering
%     \tiny
%     \subfloat[Source 1]{
%         \begin{tikzpicture}
%             \begin{axis}[mystyle]
%             \addplot [forget plot] graphics [xmin=0.5,xmax=1121.5,ymin=0.5,ymax=273.5] {Chapters/05_Separation_Known/figures/src1-1.png};
%             \end{axis}
%         \end{tikzpicture}%
%     }\hfill
%     \subfloat[Source 2]{
%         \begin{tikzpicture}
%             \begin{axis}[mystyle]
%             \addplot [forget plot] graphics [xmin=0.5,xmax=1121.5,ymin=0.5,ymax=273.5] {Chapters/05_Separation_Known/figures/src2-1.png};
%             \end{axis}
%         \end{tikzpicture}%
%     }
%
%     \subfloat[Mixture]{
%         \begin{tikzpicture}
%             \begin{axis}[mystyle]
%             \addplot [forget plot] graphics [xmin=0.5,xmax=1121.5,ymin=0.5,ymax=273.5] {Chapters/05_Separation_Known/figures/mix-1.png};
%             \end{axis}
%         \end{tikzpicture}%
%     }
%
%     \subfloat[Mix. warped by Pitch 1]{
%         \begin{tikzpicture}
%             \begin{axis}[mystyle]
%             \addplot [forget plot] graphics [xmin=0.5,xmax=1121.5,ymin=0.5,ymax=273.5] {Chapters/05_Separation_Known/figures/mixwrpedsrc1-1.png};
%             \end{axis}
%         \end{tikzpicture}%
%     }\hfill
%     \subfloat[Mix. warped by Pitch 2]{
%         \begin{tikzpicture}
%             \begin{axis}[mystyle]
%             \addplot [forget plot] graphics [xmin=0.5,xmax=1121.5,ymin=0.5,ymax=273.5] {Chapters/05_Separation_Known/figures/mixwrpedsrc2-1.png};
%             \end{axis}
%         \end{tikzpicture}%
%     }
%
%     \subfloat[Target 1 warped]{
%         \begin{tikzpicture}
%             \begin{axis}[mystyle]
%             \addplot [forget plot] graphics [xmin=0.5,xmax=1121.5,ymin=0.5,ymax=273.5] {Chapters/05_Separation_Known/figures/src1wrped-1.png};
%             \end{axis}
%         \end{tikzpicture}%
%     }\hfill
%     \subfloat[Target 2 warped]{
%         \begin{tikzpicture}
%             \begin{axis}[mystyle]
%             \addplot [forget plot] graphics [xmin=0.5,xmax=1121.5,ymin=0.5,ymax=273.5] {Chapters/05_Separation_Known/figures/src2wrped-1.png};
%             \end{axis}
%         \end{tikzpicture}%
%     }
%
%     \subfloat[Target 1 unwarped]{
%         \begin{tikzpicture}
%             \begin{axis}[mystyle]
%             \addplot [forget plot] graphics [xmin=0.5,xmax=1121.5,ymin=0.5,ymax=273.5] {Chapters/05_Separation_Known/figures/src1unwarped-1.png};
%             \end{axis}
%         \end{tikzpicture}%
%     }\hfill
%     \subfloat[Target 2 unwarped]{
%         \begin{tikzpicture}
%             \begin{axis}[mystyle]
%             \addplot [forget plot] graphics [xmin=0.5,xmax=1121.5,ymin=0.5,ymax=273.5] {Chapters/05_Separation_Known/figures/src2unwarped-1.png};
%             \end{axis}
%         \end{tikzpicture}%
%     }
%     \caption{Example of pitch variation informed NMF in the warped domain. \textit{Time} is shown on horizontal axes. \textit{Frequency} is shown on vertical axes.}
%     \label{fig:warpingdemo}
% \end{figure}

\section{Extending Pitch Variation Informed Separation}

\marginpar{This results of this extension were submitted and evaluated in SiSEC 2015 and SiSEC 2016.}

In the previous section, we showed the effectiveness of a pitch variation informed separation system on our constrained unison source separation scenario.
In the following section I show how the method can be extended to the scenario of separating the vocals/lead from the accompaniment in professionally produced music.

% from zafar
Hereby I make use of one particularity of harmonic lead sources like singing voice.
It is their ability to produce vibration using vocal folds, further filtered by the vocal tract.
As a consequence, sung melodies are \textit{mostly} harmonic and therefore have a fundamental frequency.
If one can track the pitch of the vocals, one can then estimate the energy at the harmonics of the fundamental frequency and reconstruct the voice.

Such method is summarized in Figure~\ref{fig:methods_harmonicity}.
In a first step, the objective is to get estimates of the time-varying fundamental frequency for the lead at each time frame.
This can done by a suitable pitch detection method like~\cite{de2002yin}.
A second step then is respect is then to track this fundamental frequency over time, in other words, to find the best sequence of estimates, in order to identify the melody line.
Such algorithms typically assume that the lead corresponds to the harmonic signal with strongest amplitude.
In this extension, I want show how the pitch variation informed separation system can be used in combination with a predominant melody estimation algorithm to extract singing voice from music.
\par
In a first step, the ``Melodia'' algorithm~\cite{salamon12} is used to obtain an estimate of the predominant melody from the mixture.
The mixture is then time warped based on the fundamental frequency of the melody so that it’s predominant solo part is nearly constant in F0. (See~\cite{salamon12} for details). The extraction is then carried out in time domain using efficient comb filtering.

\begin{figure}[htbp]
	\centering
  \includegraphics[width=\columnwidth]{Chapters/05_Separation_Known/figures/comb_filter.pdf}
	\caption{Block scheme diagram of a \textit{harmonic assumption} for vocals. In a first analysis step, the fundamental frequency of the lead signal is extracted. From it, a separation is obtained by filtering the mixture.}
	\label{fig:methods_harmonicity}
\end{figure}

\subsection{Predominant Melody Estimation}

The first step to extend the pitch variation informed separation system is to obtain a warp contour that follows the predominant source by means of extraction from the mixture (blind) or by human annotation (informed).
In the following I want to focus how to obtain such a warp contour using a predominant melody algorithm.\par

Estimating the fundamental frequency of one single source from a mixture of several sources is considered very difficult~\cite{klapuri08}.
However, in the case of vocal/accompaniment separation, we only consider one single source is considered as the lead source - usually the vocals.
This assumption often is applicable in modern popular music where such a lead source often is mixed louder than the other sources in the mix.
\par
Now, extracting the lead pitch contour is actually an ongoing field of research named ``melody estimation''.
However, compared to pitch or fundamental frequency, the term ``melody'' line  is only loosely defined.
A widely used defintion is the one from~\cite{poliner07}:

\begin{quote}
  ``...melody is the single (monophonic) pitch sequence that a listener might reproduce if asked to whistle or hum a piece of polyphonic music, and that a listener would recognize as being the `essence` of that music when heard in comparison.''
\end{quote}

For a more comprehensive overview of melody extraction methods, the reader is referred to~\cite{salamon14}.
In this work, I follow the ``Understanding without Separation'' paradigm, which is seen as an advantage of a melody extraction method without applying a source separation system first (also see Chapter 1, Section 1.4.5~\cite{salamon14}).\par

In turn I used the \emph{Melodia} algorithm, published by Salamon et al. in 2012~\cite{salamon12} as the basis to extract vocals from the mixture.\par

\emph{Melodia} consists of four parts:
\noindent\textbf{1)}: a time-frequency transformation is applied and spectral peaks are extracted.
\textbf{2)}: these form the basis of a \emph{saliency} spectrogram that is computed using a weighted sum over all frequencies. This allows emphasize the predominant/salient frequencies in the signal and is the core part of the \emph{Melodia} algorithm.
\textbf{3)}: From the saliency map, again, peaks are extracted and then connected to a melody line. This would already a good starting point for the melody estimate but usually contains way to many outliers due to the noisiness of the saliency representation.
\textbf{4)}: The melody line is post-processed using a viterbi decoder.
The purpose of it is to filter the contour by removing outliers, octave jumps and generally improves the smoothness of the contour using a number of heuristics.
It is important to note that this step is tracking the melody line of the previous step over time.
Usually this step is sensitive to the overall length of the processed mixture and often, this step is computed in a semantically meaningful segment of the mixture like a full track or a refrain.
\par
I applied the \emph{Melodia} using the implementation in Essentia~\cite{bogdanov13} with the default parameters (sample rate 22050 Hz, hop size = 3 ms, window size=46 ms).

\subsection{Source Extraction using Time Warping}

Once the predominant melody is obtained, the warp contour can be computed in the same way as described in Section~\ref{sub:time_warping} above.
In the unison scenario, however, we were globally warping the signal using a continuous warp contour.
In the case of full length tracks many parts are unvoiced and apply time warping on these segments would degrade the separation quality.
Therefore I only applied the warping on voiced parts and leave the non-vocal parts unaltered.
In order to do this, I used the built-in voice activity detection from \emph{Melodia}.
For all continuously voiced segments, I compute the warp contour from the  melody segments.
To reduce the complexity of the extraction, compared to the NMF mentioned in Section~\ref{sub:frequency_modulation}, I designed a comb filter that can extract the voice in the warped time domain.
Therefore I used a simple IIR Filter with the frequency response

\begin{equation}
  H(z) = \frac{1}{1 - 0.75^z{-P}},
\end{equation}

where \(P\) is constant --- due to the time warping --- pitch period in samples.
In order to then extract the vocals, zero-phase filtering is applied.
The extracted vocals were unwarped to linear time and the accompaniment signal is created by subtracting the estimated vocals from the mixture signals.
Each excerpt is then linearly crossfaded into the unaltered, accompaniment/mixture using a 10ms window.
To further reduce the complexity of the separation system, instead one comb filter for each excerpts, I modified the warping algorithm so that a user defined target pitch \(P\), rounded to integer, is used.
That way, the full signal is unwarped and resampled to the same pitch, which then only requires a singe comb filter to extract the signal.
A stereo signal is produced by using a signal filtering both channels individually.

The full procedure is depcited in Figure~\ref{fig:warp_demo}.

\begin{figure}
\begin{tabular}{cc}
  \includegraphics[width=65mm]{Chapters/05_Separation_Known/warp-demo/Mixture.pdf} & \includegraphics[width=65mm]{Chapters/05_Separation_Known/warp-demo/reference.pdf} \\
(a) Mixture Spectrogram & (b) Ground Truth Vocals \\[6pt]
\includegraphics[width=65mm]{Chapters/05_Separation_Known/warp-demo/Melodia.pdf} & \includegraphics[width=65mm]{Chapters/05_Separation_Known/warp-demo/Contour.pdf} \\
(c) Melody Estimate & (d) Warp Contour \\[6pt]
\includegraphics[width=65mm]{Chapters/05_Separation_Known/warp-demo/warped.pdf} & \includegraphics[width=65mm]{Chapters/05_Separation_Known/warp-demo/warped_filtered.pdf} \\
(e) Warped Mixture & (f) Filtered Mixture \\[6pt]
\multicolumn{2}{c}{\includegraphics[width=65mm]{Chapters/05_Separation_Known/warp-demo/Estimate.pdf} }\\
\multicolumn{2}{c}{(g) Vocals Estimate}
\end{tabular}
\caption{Examples of a pitch variation informed separation of the audio track \emph{Tamy - Que Pena Tanto Fa}.}%
\label{fig:warp_demo}
\end{figure}

\subsection{Results on SiSEC 2015}
\label{ssec:performance}

The algorithm has been applied to the Mixing Secret Dataset 100 (MSD100) dataset,  consisting of a total of 100 songs of different styles.
The separation results were evaluated using BSSeval~\cite{todo} and submitted to the SiSEC 2015 evaluation challenge~\cite{ono15}.
The system was ranked in the last third of the participants and scored only slightly better than RPCA based methods~\cite{huang12}.
The reason for this is that our proposed system highly depends on the melody estimation algorithm which in turns is based on the assumption that there exist a predominant melody in the mixture.
Unfortunately, the newly created MSD100 dataset was not mixed using professional mastering, resulting in vocals that are below average in loudness.
Due to the small energy, they were not detected as voiced by \emph{Melodia}, hence the warping was either not applied. Even worse, sometimes the pitch estimates were one octave off producing severe artifacts due to extreme warping.\par
On the positive side, the proposed method is of very low complexity and generally implemented in Python and has very low computational complexity.

\subsection{Improving Voiced/Unvoiced Detection using Deep Neural Networks}

As mentioned in the previous section, voice activity detection is of paramount importance in the proposed music separation system.
Therefore, I decided to evaluate if the performance of the system can be improved by using a more robust voice activity detection method as a separate preprocessing step.
\par
Shortly after I submitted the separation results to the SiSEC 2015 evaluation campaign, the whole audio community was shaken up by the recent success of deep learning throughout several audio related tasks that go beyond automatic speech recognition.
Among them are several tasks related to music information retrieval (MIR) such as singing voice detection which received major breakthroughs in 2014 and 2015~\cite{Lehner14, Lehner15, Leglaive15, schlueter15}.
\par
Locking back, at that time, I did not have enough knowledge to understand how and why deep learning method are so successful~\footnote{And even today, very few people would say that the deep learning community made significant progress regarding the understanding of DNNs.}.
Therefore, I decided to deploy a state-of-the-art singing voice detection system into my separation pipeline and evaluate just the end-to-end performance.
I chose to reimplement the system by Leglaive~\cite{Leglaive15}, since it was a good compromise between complexity (its uses hand-crafted features instead of large STFT frames) and performance.
In fact, the system reached a state-of-the-art accuracy of 91.5\% (F-measure of 91.5) for classifying frames of singing voice for the annotated \emph{Jamendo} singing voice detection dataset~\cite{ramona08}, which is an improvement of more than 10\% compared to the best performing non-DNN system.
I want to briefly describe the system published in in~\cite{Leglaive15}, for more details, the reader is referred to the original authors publications.
Also in the course of this thesis more of my work is based on the deep learning framework and will be explained in further details in Chapter~\ref{chapx, chapy}
\par
The method proposed in~\cite{Leglaive15} is based on a recurrent neural network (RNN) layer is very similar to a classical fully connected neural network, except that RNN applies the same set of weights recursively over an input sequence.
RNNs excelt at detecting structure in sequential data of arbitrary length.
This makes it ideal to model time series, however, in practice, the temporal context learnt is limited to only a few time instances, because of the vanishing gradient problem~\cite{Hochreiter98}.
To alleviate this problem, forgetting factors (also called gating) were proposed.
One of the most popular gated recurrent cells is the Long Short-Term Memory (LSTM)~\cite{Hochreiter97} cell.
Its effectiveness has been proven in various applications and LSTMs are the state-of-the-art approach for speech recognition~\cite{Graves13}.
The basic structue of such a sequential network is depicted in Figure~\ref{fig:lstm_2}.
\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{Chapters/05_Separation_Known/figures/lstm_2.pdf}
  \caption{Simpliefied Block diagram of a Long Short-Term Memory Network.}
  \label{fig:lstm_2}
\end{figure}
The input of the network is a 80 dimensional feature vector consisting of harmonically and percussively enhanced spectrograms frames of the mixture as described in~\cite{ono08}.
The output of the network is a framewise integer, indicating `1.0` for singing voice and `0.0` for \emph{unvoiced} frames.
I trained the network using a fixed number of frames from the DSD100 training dataset.
The vocal activity labels were obtained from the dataset by analysing the true vocals.
The network was created and trained using the Keras python framework~\cite{chollet17}.
I stacked up to three layers using the parameters as mentioned in~\cite{Leglaive15} but used unidirectional LSTMs instead of birectional to reduce computation complexity.
The trained network achives an accuracy of 85\% on the separate test set.\par
While the original network proposed in~\cite{leglaive15} was framed as  classification problem, I modified the sigmoid output activation and replaced it with a linear activation to get a linear gain \(g\) instead of probability.
This gain is then multiplied with the mixture signal so that segments with less energy are reduced in volume.
In experiments I found out that this helps the \emph{Melodia} algorithm to better detect vocal activity throughout an audio track and therefore yields in melody estimates with fewer errors.

\begin{figure}
  \centering
  \input{Chapters/05_Separation_Known/figures/sisec16}
  \caption{BSS Eval scores for the vocals and accompaniment estimates on the DSD100 dataset as used in~\cite{liutkus17}. Results are shown on the \emph{test} set only. Results indicate the improvements of the DNN based vocal activity detection system.}
  \label{fig:05_comparison_sto_stodnn}
\end{figure}

This DNN-optimized version of the algorithm was then compared (but not submitted) to the new the SiSEC 2016 dataset which was released in the meantime~\cite{liutkus17}.
One can see that the DNN vocal activity dection improved the vocals SDR by 1.5~dB which is considered as a significant improvements.

\section{Excursus: Improving $F0$ Estimation using warped method} % (fold)
\label{sec:method}

\input{Chapters/05_Separation_Known/stats}

\marginpar{This section is based on the work that has been published in 2015 together with my colleague Nils Werner~\cite{stoeter15icassp}.}

An informed system like the one we described in the previous section often has limitations due to the fact that the provided fundamental frequency variation estimate might not be accurate enough and therefore is subject to an upper limit.
Further, it can be assumed that this upper bound is especially relevant for a warping based system that relies on an instantanous estimation of the fundamental frequency.
\par
Such algorithms for estimating the fundamental frequency ($F0$) of a signal vary in stability and accuracy.
While we developed the framework for a pitch variation informed separation, we found that we can utilize this to also optimize fundamental frequency estimators itself.
In turn, we proposed a method which iteratively improves the estimates of such algorithms by applying in each step a time warp on the input signal based on the previously estimated fundamental frequency.
This time warp is designed to lead to a nearly constant $F0$. A refinement is then calculated through inverse time warping of the result of an $F0$ estimation applied to the warped signal. The proposed refinement algorithm is not limited to specific estimators or optimized for specific input signal characteristics. The method is evaluated on synthetic audio signals as well as speech recordings and polyphonic music recordings. Results indicate a significant improvement on accuracy when using the proposed refinement in combination with several well-known $F0$ estimators

\paragraph{Proposed System}
%
The development of novel methods for fundamental frequency estimation, performing as well as earlier methods, such as the popular correlation based \textsc{YIN} algorithm~\cite{de2002yin}, has proven challenging.
In a study~\cite{babacan2013comparative} it is stated that YIN still performs best in terms of accuracy.
Nevertheless, when using YIN or other block based algorithms, a frame length and a hop size have to be selected trading temporal resolution on one side against frequency accuracy and robustness on the other side.

Especially when the signal is polyphonic, the robustness is the most crucial aspect of a pitch estimator. In work from Mauch et al.~\cite{mauch2014pyin}, the robustness of the \textsc{YIN} algorithm is improved by probabilistic post-processing. However, besides robustness, there is a variety of use cases requiring high accuracy as well as high temporal resolution. Application in parametric audio coding~\cite{purnhagen2000hiln} requires the parameterization of pitch bends and vibratos. Furthermore, source separation algorithms aiming at the extraction of harmonic sources from the mixture can make use of an instantaneous $F0$ estimate~\cite{virtanen2008combining, stoterunison}. There are already contributions addressing the improvement of accuracy of $F0$ estimates such as~\cite{medan1991super} which introduced a non-integer similarity model or~\cite{christensen2007joint} which belongs to the group of parametric pitch estimators.

We propose to improve the output of already existing algorithms in terms of temporal resolution as well as accuracy by iterative time warping. Two other contributions already make use of time warping in the context of pitch estimation. Resch et al.~\cite{resch} proposed an instantaneous pitch estimation technique which optimizes a warping function that would lead to a constant pitch signal. Their optimization framework minimizes a cost function specifically targeted for speech signals. Azarov et al.\ have introduced an improved version of RAPT (called iRAPT1 and iRAPT2) which also uses time warping to some extent~\cite{azarov2012instantaneous} but misses an additional step as will be shown in Section~?.
Our main contribution is a time warping based refinement method that is applicable to any F0 estimate. Our method emphasizes the strengths of different estimators and thus can even help to improve their robustness. In the following, we will describe the refinement method (Section~?) and show the experimental evaluation and its results (Section~?).

Depending on the algorithm and application, there are several reasons why $F0$ estimators deliver a less than ideal performance. When the signal tested is not tonal --- like in unvoiced parts of speech --- a proper estimation is impossible. If the estimator is optimized on purely harmonic signals, inharmonicity or frequency jitter of the input signal will increase the estimation error. Many of these reasons will lead to errors on the coarse level of the estimate (like octave jumps). The fine level accuracy is mostly influenced by parameters like time and/or frequency resolution of the estimator. A signal containing rapid changes of the frequency or modulations like ``vibrato'' is therefore more affected regarding fine level error. To obtain a more accurate estimate, we propose to time warp the signal by using the coarse level estimate towards a more constant pitch. The underlying assumption here is that pitch estimators generally perform better the more constant the pitch is.
In this section, we formulate the mathematical background of the time warping and present our proposed method for obtaining a refined $F0$ estimate.

\subsubsection{Initial $F0$ estimate}
\label{ssub:initial_estimate}

The first step is to calculate an initial $F0$ estimate by using an existing pitch estimator. Note that we later require the estimate to be defined for every input sample, thus $\Pitch[n]$ may require interpolation. In our pipeline, we use linear interpolation for all estimators. $F0$ estimators, like YIN~\cite{de2002yin}, also provide a measure of confidence $c[n]$.
\par
% In the first step, we apply \emph{time warping} which refers to a strictly monotonous mapping
% of the natural or linear time scale $t$ to a warped time scale $\tau$ via a
% mapping function $\tau=w(t)$.
% The mapping between the two domains for the continuous time case then is:
% \begin{equation}\label{eq:contWarpedTime}
% \breve{x}(\tau)=x(w^{-1}(\tau)), \quad x(t)=\breve{x}(w(t))
% \end{equation}
% where $x(t)$ is the linear-time signal and $\breve{x}(\tau)$ is the warped-time signal.
% For the discrete time case, the signals in both linear-time and warped-time domains are sampled
% using a constant sample interval $T$. With sample indices $\nu$ and $n$ for the warped-time domain and linear time-domain respectively, the warping is performed by
%
% \begin{align}
% \breve{x}[\nu] &= x(\sigma[\nu]) & \textrm{ with } & \sigma[\nu] = w^{-1}(\nu T), \\
% \intertext{and the inverse warping by}
% x{}[n] &= \breve{x}(s[n]) & \textrm{ with } & s{}[n] = w(nT).
% \end{align}
%
% \subsubsection{Warp contour}
% \label{subs:warp_contour}

In our application, the warp map $w(t)$ is constructed in such a way that the instantaneous changes in frequency of the signal in the linear time domain are minimized in the warped time domain. For this, we derive the map from an estimate of the fundamental frequency $F0$.

For processing, the actual information needed is not the absolute instantaneous fundamental frequency but only its change over time. This means that the warping contour can be derived from an algorithm which may differ from the actual $F0$ estimator.

The discrete time warp map $w[n]$ is the scaled sum of the relative
frequency contour (the \emph{warp contour}) $W[n]$:
\begin{equation}
w[n]=N \frac{\sum^n_{l=0}{W[l]}}{\sum^{N-1}_{k=0}{W[k]}}  \qquad 0\leq n<N,
\end{equation}
where $N$ being the number of samples of the signal under consideration.
As stated above the full warp map $w(t)$ is then obtained by linearly interpolating $w[n]$. From the requirements for the mapping function it follows that $W[n]$ has to be greater than zero for all $n$. In the case of a perfect $F0$ estimate, the signal warped with the resulting contour would have a constant $F0$ equal to the average $\bar{W}$.

In the scope of this work, the warping is applied globally over the full length of the signals under consideration. An optional confidence measure $c[n]$ can be incorporated for a processed version of the warping contour. This ensures that the warp contour has no discontinuities that result in additional artifacts after re-sampling. If the estimator does not provide such a measure, a separate voiced/unvoiced detection algorithm can be used. To obtain a warp contour $W[n]$ from an $F0$ estimate we propose the following steps: \textbf{(A)} initialize the warp contour with $F0$ estimate $W = \Pitch$, \textbf{(B)} find contour segments with high confidence, i.e. $c[n]$ exceeds a given threshold, \textbf{(C)} linearly connect the high confidence contour segments and \textbf{(D)} set start and end of warp contour to a constant value if confidence is below threshold. That way warping according to $F0$ is applied in the regions of high confidence without significantly affecting the gaps in-between.
\par
To improve the accuracy of the $F0$ estimate, time warping is applied to the input signal $x[n]$ based on $W$. The input signal is 128-times oversampled using sinc based interpolation filters.
From $\breve{x}[n]$ a new $F0$ estimate $\breve{\Pitch_1}[\nu]$ is being calculated as in step \textbf{(A)}. The first step therefore is similar to~\cite{resch}. Additionally, a warped confidence measure $\breve{c}_1[\nu]$ can be used to convert $\breve{\Pitch_1}[\nu]$ into a warped \emph{warp contour} $\breve{W}_1[\nu]$. It is possible to linearly add $\breve{\Pitch_1}[\nu]$ to the first estimate for refinement, as it is done in~\cite{azarov2012instantaneous}. However for linear sweeps, the warped estimate is shifted in time. Thus an error is introduced which is even more distinct if the first $F0$ estimate is error prone. We therefore propose a method to reduce this error:
\begin{itemize}
	\item Inverse time warping is applied to $\breve{\Pitch_1}[\nu]$ based on the original warp contour $W$ resulting in $\Pitch_1[n]$.
	\item A refined $F0$ estimate after one iteration is then calculated by $\Pitch_1^r[n] = \Pitch_1[n] \cdot W[n] / \bar{W}$ assuming that the warp contour is initialized as in step \textbf{(A)} above.
	\item The refinement can be repeated $k$ times to obtain a better estimate. To avoid accumulating errors introduced by the re-sampling based warping, more iterations benefit from calculating a refined warp contour/warp map instead of doing a nested warping on the input signal. The map is obtained by inverse time warping of the warp contour $\breve{W}_1[\nu]$ resulting in $W_1[n]$. A refined warp contour $W_1^r[n]$ is then obtained in the same way as the refined $F0$ estimate is calculated. For the calculation of the $k$th step, time warping is based on the $W_{k-1}^r[n]$ refined warp contour.
\end{itemize}
An example of the proposed refinement is depicted in Figure~\ref{fig:teaser}. The final refined estimate is closer to the reference than the $F0$ estimator without refinement. It also shows (right plot) how much ``flatter'' the $F0$ contour becomes after each iteration.
Note that compared to~\cite{resch}, our method does not use a complex optimisation scheme but relies on the performance of the pitch estimator in successive iterations. Hence our ``black box'' like post processing simplifies the procedure such that it can be applied to any pitch estimator. That way the selection of a pitch estimator which best fits to the signal type can be seen as an optimisation.

\begin{figure}[t]
\centering
\begin{tikzpicture}
\node[inner sep=0pt] (3d) at (4.4,0) {\includegraphics[width=.4\textwidth]{Chapters/05_Separation_Known/figures/f0_3d.pdf}};
\node[inner sep=0pt] (output) at (0,0) {\includegraphics[width=.4\textwidth]{Chapters/05_Separation_Known/figures/output_f0.pdf}};
%
\end{tikzpicture}
\caption{$F0$ refinement for one excerpt of synthesized speech using \textbf{YIN}~\cite{de2002yin} with 10 iterations. \emph{Left}: Estimated $F0$ in linear time. \emph{Right}: estimates after each warping iteration in warped time.}
\label{fig:teaser}
\end{figure}

\vspace{-0.6em}
\subsection{Experiments and Evaluation} % (fold)
\label{sec:experiments}

For the evaluation of the proposed $F0$ refinement, we test the refinement algorithm with the following $F0$ estimators:\\
\textbf{YIN}~\cite{de2002yin} is used as an FFT based implementation~\cite{bogdanov2013essentia}. The confidence measure is thresholded for values lower than 0.6 on the speech recordings. \textbf{iRAPT1,2}~\cite{azarov2012instantaneous} are improved versions of the RAPT framework. We use the author's MATLAB implementation of the iRAPT1 and iRAPT2 algorithms.\ iRAPT2 is a refinement method that is comparable to our proposed method. To evaluate the results, we apply our refinement to iRAPT1 and compare it with the refinement produced by iRAPT2. $c[n] < 0.7$ is used for thresholding speech recordings. \textbf{MELODIA}~\cite{salamon2012melody} is not designed to be an $F0$ estimator but is able to extract the \emph{predominant} melody in a polyphonic mixture. We increase the bin resolution to 0.5 semitones, to increase the accuracy. We used the \textsc{Essentia} implementation. For thresholding we use the built-in voiced/unvoiced detection.
For YIN and MELODIA, we evaluate on a frame length of 64~ms and a hop size of 16~ms. For iRAPT1 and iRAPT2  we use the fixed frame length parameters of the author's implementation.
\par
We use the established evaluation measures \textsc{Gross Pitch Error} (GPE) and \textsc{Mean Fine Pitch Error} (MFPE)~\cite{azarov2012instantaneous}. We focus on MFPE in our results, measuring the absolute deviation of $F0_{\mathrm{true}}$ and the $F0$ estimate per sample.
As mentioned in~\cite{resch}, evaluating the accuracy of $F0$ estimates is challenging because of the lack of ground truth datasets annotated on a time scale with such a high resolution. Most of the available audio test data sets are not suitable because the $F0$ annotation is only available with low time resolutions. By using such a dataset there is a risk that the refined $F0$ estimate is higher in MFPE. This is because the refined estimates show more of the fine structure deviating from the coarse annotation which then is considered as piecewise constant. To address this issue, we first present the evaluation results on synthetic data. To verify our synthetic results, we present the results of speech data annotated on 10~ms frames derived from laryngograph signals. We did only evaluate and process the voiced parts of the signals as indicated in the provided annotation labels. Also note that since we focus on the MFPE, all segments where one of the estimators results in a GPE $> 0$ are excluded from the results, hence the GPE for all of our results is 0. The proposed refinement has been processed with one iteration ($k=1$). Experiments showed that more iterations only marginally improve the results.

% \vspace{-0.6em}
% \subsubsection{Oracle Refinement}
%
Since the proposed refinement algorithm repeatedly applies pitch estimation, the performance of these estimators on the time warped (nearly constant) signal is of interest. Therefore we included the results of an oracle refinement where the first estimate is set to a ground truth pitch. Additionally this also does reveal information about the quality of the ground truth annotation itself.

\vspace{-0.6em}
\subsubsection{Synthetic Data} % (fold)
\label{ssub:sythetic_data}

To generate synthetic test data we use pitch label annotations of the PTDB-TUG speech data set~\cite{pirker2011pitch}. We synthesize the melody or voice using a simple sinusoidal signal model. To get accurate ground truth data, the pitch annotations were up-sampled to audio rate by using linear interpolation for the PTDB-TUG. Similar to~\cite{mauch2014pyin}, we then synthesized the data using cosine based oscillators adding 10 harmonics to each signal output.
The test set has been rendered at 16 kHz. The complete PTDB-TUG set results in almost 10 hours of input signal data.
We present the results of the synthetic data as box plots in Figure~\ref{fig:ptdbtug_synth} grouped by estimator. It shows that all estimators benefit from the refinement in terms of MFPE. The iRAPT1 estimator shows the best improvement of \ptdbtugsynthiRAPTIMPROMFPE \% in MFPE. As expected, the Oracle Refinement is almost at 0 MFPE.

\begin{figure}[t!]
\centering
		\includegraphics[width=0.90\columnwidth]{Chapters/05_Separation_Known/figures/stats_boxplot_ptdb_synth.pdf}
\caption{Results from the synthesized PTDB-TUG dataset. MFPE grouped by estimator. Solid/dotted lines represent medians/means. Outliers are not shown.}
\label{fig:ptdbtug_synth}
\end{figure}

\vspace{-0.6em}
\subsubsection{Speech Data} % (fold)
\label{ssub:real_data}
For the results of the algorithm on real data we first used the same PTDB-TUG items as in the synthetic data but processed the accompanying speech recordings. The MFPE values were then calculated by averaging the sample wise $F0$ estimates from our proposed method over frame lengths of 10~ms to match the annotation data. The results are shown in Figure~\ref{fig:ptdbtug_real}. The mean values indicate that the MELODIA algorithm performs best overall. We can see that the refinement does not show a clear effect on the iRAPT estimator. The oracle refinement results indicate that even if a ground truth is known, the refinement based on the warped (constant) signal can not get much lower in MFPE. As also seen on synthetic data, iRAPT2 does not show any significant improvements compared to our proposed refinements.

\vspace{-0.6em}
\subsubsection{Polyphonic Mixtures} % (fold)
\label{ssub:real_data}

Pitch estimation of polyphonic mixture input signals in general is known to be more difficult than on monophonic signals. To show that our proposed refinement is not bound to the optimisation on specific signals we processed the \mbox{MedleyDB}~\cite{MedleyDB} which consists of 108 professionally recorded music mixes where the main melody has been annotated by humans. We only evaluate the MELODIA~\cite{salamon2014melody} estimator in this scenario. Frame lengths and hop sizes were increased to 92~ms and 23~ms, respectively. The set is processed at 44.1~kHz. To further back up the results of the fine pitch error in this scenario, we additionally evaluated the results of a correlation based measure as introduced in~\cite{resch} (See Equation (19)). Instead of computing the correlation coefficients on the mixture, we used the accompanying multi-tracks. The track which most predominantly contributed to the main melody has been chosen for the correlation coefficient measure. The results of the experiment are shown in Figure~\ref{fig:medley}.

\begin{figure}[t!]
\centering
		\includegraphics[width=0.90\columnwidth]{Chapters/05_Separation_Known/figures/stats_boxplot_ptdb_real.pdf}
\caption{Results from the real recordings PTDB-TUG dataset. MFPE grouped by estimator. Solid/dotted lines represent medians/means. Outliers are not shown.}
\label{fig:ptdbtug_real}
\end{figure}

\begin{figure}[t]
\centering
		\includegraphics[width=0.90\columnwidth]{Chapters/05_Separation_Known/figures/stats_boxplot_medley.pdf}
\caption{Results from the real recordings MedleyDB dataset. MFPE and Correlation Coefficient grouped by estimator. Solid/dotted lines represent medians/means. Outliers are not shown.}
\label{fig:medley}
\end{figure}

\section{Discussion and Scope}

% start zafar
While methods focusing on harmonic models for the lead often fall short in their expressive power for the accompaniment, the methods we reviewed in this section are often observed to suffer exactly from the converse weakness, namely they do not provide an adequate model for the lead signal. Hence, the separated vocals often will feature interference from unpredictable parts from the accompaniment, such as some percussion or effects which occur infrequently.

Furthermore, even if the musical accompaniment will exhibit more redundancy, the vocals part will also be redundant to some extent, which is poorly handled by these methods. When the lead signal is not vocals but played by some lead instrument, its redundancy is even more pronounced, because the notes it plays lie in a reduced set of fundamental frequencies. Consequently, such methods would include the redundant parts of the lead within the accompaniment estimate, for example, a steady humming by a vocalist.
% end zafar

Future work could include a robust multi pitch variation estimator for musical instruments. Salamon and Gomez~\cite{salamon12} describe the current state of the art of f0 estimation. Some approaches use source separation to estimate multiple f0 pitch tracks. Therefore our approach shows that a robust multi pitch f0 estimate can also help to improve source separation. In the future an iterative multi-step procedure could lead to better results in both problem domains.\par
