\chapter[Experiments on Estimating the Number of Sources]{One, Two, Three, Many: Perceptual Experiments on Estimating the Number of Sources}
\label{cha:countanalysis}
\bigskip
\bigskip
\bigskip
\bigskip
%
The separation of mixtures into its original audio sources, as presented in the previous chapters, is a challenging task.
Furthermore, source separation is difficult to evaluate, and researchers compare to a known reference even though this does not reflect how humans separate mixtures.
It has long been a research topic to answer the question \emph{if} humans can separate, by fully extracting the desired source or if we can focus our attention on one source --- segregate them~\cite{bregman90}.
Moreover, despite recent progress in auditory science~\cite{carlyon04, koelsch05, rabinowitz13}, research still is investigating how separation takes place in the auditory cortex or other parts of the human brain.
One thing, however, we can assess directly is if humans can reliably detect the number of sources in a mixture of several sources.
\par
In order to address this question, it helps to understand how humans infer counts and if our strategy is depending on the count.
When we look at vision, these are questions that have already been discussed over a hundred years ago in scientific research; an early study in the field of psychology of vision was published by Jevons in 1871~\cite{jevons1871}.
Jenvons presented an experiment to quickly infer the number of objects (beans) and came up with the hypothesis that humans can instantly estimate the number of objects without actually counting and therefore identifying them.
Jenvons mentioned in~\cite{jevons1871}:

\begin{quote}
``It is well known that the mind is unable through the eye to estimate any large number of objects without counting them successively. A small number, for instance, three or four, it can certainly comprehend and count by an instantaneous and apparently single act of mental attention.''
\end{quote}

This ``one-two-three-many'' hypothesis was a fundamental observation.
The fact that we can directly infer the numerosity of small numbers of objects, up to about four, is also known as \emph{subitizing}~\cite{kaufman49, burr10}. We refer to this strategy as ``direct count estimation''.
Concerning our hearing, there are indications that the auditory system is capable of subitizing audio sources~\cite{hoopen79}.
And surprisingly, as shown in~\cite{kashino96, kawashima15}, humans share the same limitations of correctly estimate up to three simultaneously active speakers.
\par
In this chapter, we present two experiments contributing to this interesting field of research.
The first experiment (Section~\ref{sec:ismir}) addresses the question if the number of instruments in polyphonic music is subject to the same limitations.
In the second contribution (Section~\ref{sec:count_experiment}) the aim is to verify the findings of an earlier experiment in~\cite{kawashima15} and increase the number of stimuli to be used for comparison of machine learning based count estimations methods

\section{Instrument Count Estimation}%
\label{sec:ismir}

\marginpar{This chapter was previously published in~\cite{stoeter13} and has been revised for this thesis.}

Decomposing music into its original audio sources can be a challenging task. Source separation methods can be analyzed how well they perform mathematically, but a human versus machine comparison is cumbersome because measurement of the human separation performance is problematic. One can easily evaluate if humans can detect the number of sources in a mixture of several sources. However, there are indications that even for this task, humans tend to fail if more than three sources are present at the same time \cite{huron89}. Therefore, we want to take a first step by designing an experiment where we focus on polyphonic music of inhomogeneous timbre, where the question is: What is the number of instruments humans can estimate correctly? Several results are addressed in this paper including a possible upper limit of the number of perceived instruments, but also if one can see significant differences in performance of musicians compared to non-musicians. Such results can be used in auditory modeling or as a pre-processing step for source separation algorithms.
This paper presents the results of an experiment, a detailed description of the stimuli and the statistical methods that were used.
\par
% from ICA!
The perception of concurrent sound sources has been analyzed on different scales so far. Bregman's and McAdams'~\cite{mcadams89} auditory stream theory can be seen as an analytical way of describing how sound events are perceived by the human auditory system. Unfortunately, it is difficult to model professionally produced music by auditory stream models because of its high complexity.
However, none of them is motivated to model the perceived number of musical sources. Kashino et. al \cite{kashino96} addresses the questions for concurrent speakers in a ``cocktail party'' like the environment and found an upper limit of three voices humans can perceive. When the focus shifts to musical instruments as sources, research has to take concepts from musicology into account. Huron \cite{huron89} was the first who addressed this question in 1989 at a musically meaningful level. Huron asked for the number of voices within a piece of music, whereby voices in musicology one can define it as a line of sound or note events (See~\cite{Cambouropoulos2008} for further definitions). Huron determined by experimental results that the number of correctly identified voices is up to three.
We wanted to take a first step by designing an experiment where we focus on polyphonic music of inhomogeneous timbre, where the question is: What is the number of instruments humans can estimate correctly?
Several results are addressed in this work include a possible upper limit of the number of perceived instruments, but also if one can see significant differences in performance of musicians compared to non-musicians.

\subsection{Experiment}
% ica
For the purpose of gaining more knowledge in understanding the human perception of multiple present instruments, an experiment was conducted. Huron selected voices from organ pieces only. We wanted to address the more general case where voices are played by different instruments.
As we set our focus on comparison between musicians and non-musician.
Our experiment was designed so that it respects the fact that the latter has an only limited musical background.
\par
% ica
Although it might be interesting to have direct comparison with Huron's experiment, we agree that expanding the methods to an inhomogeneous timbre case is error prone. One reason is that there is reasonable doubt about the non-musicians understandings in terms of how a voice is defined. This is why we choose a trade-off with a more simplified experiment where we asked for the number of instruments instead of voices. Also whereas Huron \cite{huron89}  excluded subjects from his experiment because of their lower performance, we compared the results of both groups.

\subsection{Stimuli}
% ica
The selection of music items is crucial for our experimental setup. Usually music recordings have no ground truth metadata available to determine the actual number of instruments. Using annotated music like that from the RWC database~\cite{rwc} fulfills this requirement but lacks the possibility to remix, attenuate or suppress specific sources. This is important so that the experiment consists of equally grouped stimuli. Instead of the original RWC recordings, the annotated MIDI data itself was used as prototypes for the stimuli.
% ica
To make the count estimation task less ambiguous for the subjects, the instrumentation was chosen to be mostly constant during the music piece.
Therefore we calculated an ``instrumental stationarity'' metric.
The annotated MIDI files from \cite{rwc} were converted into piano roll representations for each instrument channel. This representation was then converted into a binary \emph{instrumentation activity matrix} $\mathbf{I}_{AM}\lbrack \mathbf{\underline{k}}_1 \vert \mathbf{\underline{k}}_2 \vert  ... \vert \mathbf{\underline{k}}_N \rbrack \in \{0,1\}$, where at each discrete time instant $i$ a vector $\mathbf{\underline{k}}_i$ indicates which instruments are active. The aim is then to select frames of length $N$ which are stationary by means of changes in instrumentation and activity. To get many items with a high instruments count, the maximum number of instruments within a frame was stored in a binary mask $\mathbf{\underline{k}}_{max}$ which was compared with all $\mathbf{\underline{k}}_{i=1...N}$ so that $(\vert\mathbf{\underline{k}}_{i} \oplus \mathbf{\underline{k}}_{max}\vert \leq 1) \lor (\mathbf{\underline{k}}_{i} = \underline{0})$. The resulting binary vector was smoothed with an averaging kernel of size $N$. By peak picking we got a list of possible candidates which contained a high stationarity in instrumentation.
Further the RWC files were filtered a priori to exclude items dominated by electronic instruments or singing voice. Table~\ref{tab:items} presents the selected 12 items representing pairs of one to six simultaneously present instruments. Each item is around seven seconds long. By cutting at note offsets we varied the lengths of the items to make it semantically more meaningful. Six items (notated as RM-C***) belong to the classical western music genre whereas the other items are of mixed genre.
\par
The MIDI files were humanized randomly and rendered in a professional sequencer software utilizing state-of-the-art commercial sampling products.
The process is similar to the dataset creation mentioned in Section~\ref{sec:unison_dataset}.
The rendered files were processed with convolutive reverb to match the original recordings. 
Additionally a loudness normalization was applied according to EBU-R128\cite{EBU2011}. To avoid spatial cues every item were rendered to mono at 16 bit/44.1 kHz.

\begin{table}[htb]
\center
\scriptsize
\begin{tabular}{cr@{.}lr@{.}lp{6cm}c}
\toprule[1.5pt]
RWC~ID & \multicolumn{2}{c}{Start~[s]} & \multicolumn{2}{c}{Dur.~[s]} & Instrumentation & \(k\)\\
\midrule
J021 & 46 & 5 & 6 & 6 & Piano, Contrabass~(pizz.) and Trumpet & 3\\
\hline
C001 & 0&0 & 9&0 & Bassoon & 1  \\
G047 & 35&3 & 8&3 & Violoncello & 1 \\
\hline
C016 & 0&9 & 7&6 & Viola and Violoncello & 2\\
G068 & 132&4 & 6&6 & Violin and Flute &  2\\
\hline
C018 & 240&4 & 5&4 & French~Horn, Piano and Violin & 3\\
G046 & 0&3 & 7&9 & Contrabass, Piano and Violoncello & 3\\
\hline
C013 & 5&6 & 6&0 & Flute, Viola, Violin and Violoncello & 4\\
G036 & 0&0 & 6&5 &  Acoustic~Guitar, Electric~Bass, Piano  and Violin & 4\\
\hline
C012 & 112&0 & 6&0 & Contrabass, Flute, Viola, Violin and  Violoncello & 5\\
G037 & 67&1 & 7&0 & Acoustic~Guitar, Contrabass~(pizz.), Flute, Piano and Tenor~Sax & 5\\
\hline
C001 & 147&8 & 6&0 & Bassoon, Clarinet, Contrabass, French~Horn, Oboe and Violin & 6\\
G028 & 17&5 & 6&5 & Electric~Bass, Electric~Guitar, Flute, Piano, Trombone and Trumpet & 6\\
\bottomrule[1.5pt]
\end{tabular}
\caption{Selected items from the RWC Music Database \cite{rwc}. Item \emph{J021} is used as training item.}
\label{tab:items}
\end{table}

\subsection{Methods and Participants}
The experiment was attended by 62 participants, where half of them regularly play a musical instrument. They were asked to count how many different instruments they can hear. 12 items from the test set (Table~\ref{tab:items}) were played back in random order. The experiment was presented by a user interface depicted in Figure~\ref{fig:experiment_ui}. Except for the training item, every subject could play back each stimulus up to three times. Additionally they were asked to estimate how certain they were in their decision (ranged from \emph{uncertain} to \emph{very certain}).
\begin{figure}[h]
    \centering
        \fbox{\includegraphics[width=3in]{Chapters/07_Analysis_Experiment/figures/user_interface.png}}
    \caption{Experiment User Interface}
    \label{fig:experiment_ui}
\end{figure}
Instead of a slider UI-element, the interface only features plus and minus buttons so that the subjects were not biased about the maximum number of instruments. Item \emph{J021**} had been selected as a training item and was presented to the subjects during the introduction phase to make them familiar with the user interface. This trial also unveiled the number and name of the instruments within that piece. After they had read the introduction page, the subjects were asked to adjust the volume during the training example to their preference and leave the volume at that level for the duration of the experiment. The stimuli were presented on \textsc{Beyerdynamics DT770} headphones connected to a \textsc{RME Babyface}. The complete test took about 20 minutes on average for every participant.

\subsection{Results: A gap of one instrument}

\begin{figure}[h]
\centering
\begin{minipage}{0.8\textwidth}
\begin{tabular}{c}
    \includegraphics[width=\textwidth]{Chapters/07_Analysis_Experiment/images/error_means.pdf}
\end{tabular}
\end{minipage}
\\
\begin{minipage}{0.8\textwidth}
\begin{tabular}{c}
    \includegraphics[width=\textwidth]{Chapters/07_Analysis_Experiment/images/error_diff.pdf}
\end{tabular}
\end{minipage}
\caption{Error probability (top) and Mean of $\Delta = I-R$ (right) categorized by the number of instruments.}
\label{fig:meanerror}
\end{figure}

The independent variable $I(i)$ is the number of instruments of one music item $i$ where in this case $I(i) \in \{1,2,...,6\}$. $R(i,s)$ is defined as the number of instruments that are perceived and counted by subject $s$ for music item $i$. The dependent variable is the derived from the main subject response as $\Delta(i,s) = I(i) - R(i,s)$ transformed into a binary scale:
\begin{align}%
\label{eq:response}
    E(i,s)&=\begin{cases}
        0 & \text{if $|\Delta| = 0 $ } \\
        1 & \text{if $|\Delta| > 0 $ .}\\
    \end{cases}
\end{align}
\par
The primary statistical null hypothesis ($H_1$) is stated in that the means of $\Delta$ and $E$\footnote{The fact that $E$ is dichotomous will lead to a mean value that equals to a probability of a binary distribution.}, grouped by the number of instruments, do not differ significantly. As we also want to test the between-groups performance of musicians versus non-musicians, we introduce another dependent variable $M(s) \in \{0,1\}$ of binary scale. This is stated in a secondary null hypothesis ($H_2$) where the means of $\Delta$ and $E$ are not significantly different between musicians and non-musicians.
No subjects were screened from the results, although there are two cases where no valid response had been made. Results are grouped by items of $I$ instruments.
\par
In general, participants tended to perform worse for items with more than two instruments. The probability of correctly estimating one instrument was 90.0\% whereas only one person out of 62 gave a correct response for an item with six instruments. In some cases, the number of instruments does not correspond to the number of voices for every item. Items where an instrument plays more than one voice and voices which are played by more than one instrument. However, most of the chosen instruments are monophonic so in our case, this occurred only for items where piano or guitar is present. Also, we made sure that the number of total voices did not exceed the maximum number of instruments in that item. Voices being played by more than one instrument (unison), which is present in G068, that showed surprisingly good results.

\subsubsection*{Underestimation}

We confirmed the results in~\cite{huron89} that the most common error is the underestimation of one instrument, although this accounts only for 43 \% of the responses in our experiment. Only in one case $\Delta$ is negative (overestimation) which is item C016, a ``Clarinet Quintet in A major by Wolfgang Amadeus Mozart (K.581. 1st mvmt)'' where we have excluded the solo clarinet part and two strings. Still, the remaining sound seems to be so similar to that of a quartet that musicians tended to hear ``phantom'' instruments.

\subsubsection*{Self-Evaluation}

Figure~\ref{fig:certainty} shows the results of the subjects certainty grouped by instrument count. Although the rate of ``very certain'' responses drops down to 11.3\% for items with six instruments the rate of ``certain'' responses is still as high as 43.5\%. When we take $\Delta$ into account we find a significant linear correlation between $\Delta$ and certainty where 0 is uncertain and 2 is very certain (Pearson's $r = -0.227$ at the $p=0.05$ level).

\begin{figure}[h]
	\centering
		\includegraphics[height=2.5in]{Chapters/07_Analysis_Experiment/images/certainty.pdf}
	\caption{Responses for certainty of subjects by number of instruments}
	\label{fig:certainty}
\end{figure}

\subsubsection*{Main Effects}

To test the null hypotheses ($H_1$ and $H_2$), statistical tools are required.
The first tests focus on $\Delta$ which is an interval-scaled variable. To show differences between means of two or more groups, usually, One-Way-ANOVA tests are applied. ANOVA tests expect independent normally distributed variables and homogeneity of the variances in each group. However both the Kolmogorov--Smirnov test of normal distribution and Levene's test to determine the homogeneity of group variances fail. In such cases variables scaled like $\Delta$ could be transformed so that the boundaries are straightened out. A typically used $arcsin(\sqrt{\Delta})$ transformation was applied to $\Delta$ resulting in slightly higher $p$ values but still not statistically significant. Although ANOVA is known to be robust enough to run the tests against non-normal distributed cases and unequal variances, the significance levels of the results are doubtful. Therefore we choose to run a non-parametric test. The Kruskal--Wallis test can be applied even if the data is not normally distributed. However, it has to be run on a slightly modified hypothesis which compares the medians of groups instead of the means. The Kruskal--Wallis test allows to reject both modified hypotheses (asymptotic $p = 0.000$, $\chi^2 = 499636$, $df=5$).
\par
Concerning $E$ which is a categorical variable, linear models such as ANOVA cannot be used.
As described in~\cite{jaeger08}, instead, a binary regression model that turns the mean of $E$ into a binomial distributed probability can be used.
Similar to ANOVA, the output variable will be modeled by a \emph{binary logit regression} that models the output using \emph{log linear} values.
\par
By including the main factors $I$ and $M$ we set up a \emph{Generalized Linear Model} (GLM)
\begin{equation}
    logit(E) =  \text{Intercept} + x_1 \text{I} + x_2 \text{M} .
    \label{eq:logit_main}
\end{equation}
A test of the main effects is statistically significant ($\chi^2$ = $437418$, $p < 0.000$, $df = 6$) so that both null hypotheses ($H_1$ and $H_2$) can be rejected. The significance of both effects as well as parameter estimates and Wald values of the calculated model are shown is shown in~\cite{stoeter13}.
\par
The results indicate that there is a significant difference in the error probability for groups of instrumentation counts but also for musicians versus non-musicians. A pairwise comparison test based on the mean differences reveals where these differences are located. Regarding the error probability of different instrument counts, the pairwise comparison test reveals that nearly all groups show significant mean differences between each other, which was the expected result. However, by calculation using the logit GLM model shown in equation~\ref{eq:logit_main} we found that there are two groups of items of five and six instruments (mean difference $0.04$, std. error = $0.019$, $df = 1$, $p = 0.055$) that did not show any significant difference. For both groups, the error probability is close to 100\%.
\par
To investigate the difference in performance between musicians and non-musicians a pairwise comparison between those two groups was run. Overall musicians perform about 20\% better throughout the test (mean difference = $0.18$, std. error = $ 0.0044$, $df = 1$, $p=0.000$). We do not know what caused these differences as the level of professionalism had not been surveyed. Also, 37 \% of the musicians additionally had experience in audio engineering due to their profession.
\par
Further, to look at possible interaction effects between the number of instruments and the groups of musicians and non-musicians we adapted our logit equation to
\begin{equation}
    logit(E) =  \text{Intercept} + x_1 \text{I} + x_2 \text{M} + x_3 \text{M}\times\text{I} .
    \label{eq:logit_interactions}
\end{equation}
We then reran the GLM analysis selecting only items of three and four instruments. This avoids quasi-complete separation in the logit regression model which is caused by low variances in the error probability for items of $I \in \{1,2,5,6\}$.
The model effects of the subset can be found in~\cite{stoeter13}.

\begin{figure}[h]
    \centering
    \scriptsize
    \begin{tikzpicture}[node distance=2cm]
        \draw [-,thick] (0,5*1.25) node (yaxis) [left] {$ $}
            |- (7,0.75) node (xaxis) {$ $};

        \draw (-1.1, 5) node[left,rotate=90] {Error~Probability};

        \draw (1,-0.2+0.75) -- (1,0+0.75) node[below=4pt] {musician};
        \draw (5.5,-0.2+0.75) -- (5.5,0+0.75) node[below=4pt] {non-musician};

    %    \draw (-0.1, 0) -- (0.1, 0) node[left=4pt] {$0\%$};
        \draw (-0.1, 1.3710*1.25) -- (0.1, 1.3710*1.25) node[left=4pt] {$0.27$};
        \draw (-0.1, 3.2258*1.25) -- (0.1, 3.2258*1.25) node[left=4pt] {$0.65$};
        \draw (-0.1, 4.3548*1.25) -- (0.1, 4.3548*1.25) node[left=4pt] {$0.87$};
    %    \draw (-0.1, 5*1.25) -- (0.1, 5*1.25) node[left=4pt] {$100\%$};

        \draw[dashed,style={color=black!35}] (0, 1.3710*1.25) -- (1.5, 1.3710*1.25) node[left=4pt] {};
        \draw[dashed,style={color=black!35}] (0, 3.2258*1.25) -- (1.5, 3.2258*1.25) node[left=4pt] {};
        \draw[dashed,style={color=black!35}] (0, 4.3548*1.25) -- (5.5, 4.3548*1.25) node[left=4pt] {};

        \GraphInit[vstyle=Normal]
        \tikzset{EdgeStyle/.style={}}
        \tikzset{colorstyle/.style={
            shape= circle,
            line width = 1pt,
            draw = black,
            fill = #1!30}
        }

        \Vertex[L=3,x=1,y=1.3710*1.25,style={colorstyle=red,line width = 4pt}] {3M}
        \Vertex[L=4,x=1,y=3.2258*1.25,style={colorstyle=blue,line width = 2pt}] {4M}
        \Vertex[L=3,x=5.5,y= 3.2258*1.25,style={line width = 4pt}] {3N}
        \Vertex[L=4,x=5.5,y=4.3548*1.25,style={colorstyle=green, draw = black!0,line width = 0}] {4N}

        \Edge[label=$0.23$,style={color=black!50}](3N)(4N)
        \Edge[label=$0.37$,style={color=black!50}](3N)(3M)

        \Edge[label=$0.23$,style={color=black!50}](4N)(4M)
        \Edge[label=$0.37$,style={color=black!50}](4M)(3M)
        \Edge[label=$0.60$,style={color=black!50}](3M)(4N)
        \Edge[label=$0.00$,style={color=red, line width = 2pt}](3N)(4M)

        \end{tikzpicture}
        \caption{Pairwise comparison between the interaction of Musician/Non-Musician and the number of instruments (labeled in nodes). The costs between nodes indicate the mean differences between groups. The red/bold line indicates that is there is no significant difference at the $p=0.005$ level.}
        \label{fig:graph}
\end{figure}
The results indicate that the interaction of Musician$\times$Instruments is not significant on a $p=0.05$ level in general and a pairwise comparison test reveals two groups of equal probability. The pairwise comparisons are depicted in Figure~\ref{fig:graph}. The red vertex indicates there is no significant difference in the error probability for the group of musicians in items with four instruments compared to non-musicians in items of three instruments. Therefore a gap in the error probability of one instrument between those two groups becomes apparent.
\par
This experiment shows that instrument count estimation tasks in music is a difficult task for humans. Our experiment with 62 participants was conducted to address the question of how many instruments one can estimate correctly.
The focus was set on stimuli of inhomogeneous timbre and also mixed genre. By comparing musicians to non-musicians, we revealed that there is a significant difference in performance. Particularly this gap is most prominent for items of three and four instruments. Furthermore, for all stimuli (ranging from one to six instruments) we see that musicians performed about 20\% better than non-musicians. The experiment shows an assumed upper limit for items with more than three instruments.

\subsection{Experiment at Scale}

Many tasks in auditory experiments such as quality assessment~\cite{recommendation2001MUSHRA}, are not suitable for untrained participants, limited resources (low quality headphones, noisy environments) or time constraints.
We found that an experimental design such as count estimation, where only a single number is asked to the participant, is an ideal experimental environment to evaluate scale in an uncontrolled environment such as on the web.
We therefore designed a follow up study, published in~\cite{schoeffler13} that compares a large scale web experiment to our  laboratory results, similar to previous comparisons for other auditory tasks~\cite{Welch1996, lee2010, Salganik2006, Reips2012}.
\par
We used the same stimuli as in the laboratory experiment, however, the training phase was slightly shortened.
The experiment took place between February 2013 and April 2013 where participants visited the experimental website\footnote{{\scriptsize\url{http://www.audiolabs-erlangen.com/experiments/wice/}}}.
After a screening procedure, described in~\cite{schoeffler13}, a total of 1168 valid participants remained.
\par
The main results of the web based experiment in comparison to the previous (lab-bases) experiment is depicted in Figure~\ref{figure:error_probability_iis_vs_web}.
The figure shows the mean probability of correct responses for both environments.
The result indicate that there a only very small differences between both experiments. 
In fact, a detailed analysis in~\cite{schoeffler13} revealed that there are no statistically significant differences between the results of the two experiments. 

\begin{figure}[t]
\centering
\begin{tikzpicture}

\begin{axis}[
xlabel={Number of Instruments},
ylabel={Mean of $\textit{Resp}_{\mathrm{Correct}}$},
legend style={
font=\tiny,
ymax=1.1,
legend pos=north east,
},
legend cell align=left
]

\addplot[color=red,mark=triangle,dash pattern=on 1pt off 1pt] plot file {Chapters/07_Analysis_Experiment/plotdata/error_prob_iis_musicians.data};
\addlegendentry{Musicians [lab]}

\addplot[color=blue,mark=o,dash pattern=on 1pt off 1pt] plot file {Chapters/07_Analysis_Experiment/plotdata/error_prob_iis_non_musicians.data};
\addlegendentry{Non-Musicians [lab]}

\addplot[color=black,mark=square,dash pattern=on 1pt off 1pt] plot file {Chapters/07_Analysis_Experiment/plotdata/error_prob_iis_all.data};
\addlegendentry{All [lab]}


\addplot[color=red,mark=triangle] plot file {Chapters/07_Analysis_Experiment/plotdata/error_prob_web_musicians.data};
\addlegendentry{Musicians [web]}

\addplot[color=blue,mark=o] plot file {Chapters/07_Analysis_Experiment/plotdata/error_prob_web_non_musicians.data};
\addlegendentry{Non-Musicians [web]}

\addplot[color=black,mark=square] plot file {Chapters/07_Analysis_Experiment/plotdata/error_prob_web_all.data};
\addlegendentry{All [web]}


\end{axis}
\end{tikzpicture}
\caption{Probability of $\textit{Resp}_{\mathrm{Correct}}$ grouped by Internet experiment (web) and laboratory experiment (lab). Solid lines represent the results of the Internet experiment and dashed lines represents the results of the laboratory experiment. Figure from~\cite{schoeffler13}.}
\label{figure:error_probability_iis_vs_web}
\end{figure}

Further analysis in~\cite{schoeffler13} revealed that ``the participants in the laboratory experiment were about 4.6\% better in average for all stimuli than the participants of the Internet experiment. When looking into the differences between musicians and non-musicians, the outcome for the Internet experiment and laboratory experiment differ slightly. In the laboratory experiment musicians performed about 31.6\% better than non-musicians and in the Internet experiment musicians performed about 20.85\% better.''
Given a hypothesis that musicians are generally better at performing  certain musical related tasks, it indicates that in the (non-anonymous) laboratory experiment, participants that answered to be a musician were, on average, more professional than in the web based experiment.
\par
Finally, the experiment also showed that humans are able to correctly estimate a count even in a very challenging scenario such as unison mixtures and when asked in a not optimal environment.
In fact, the results showed that ``$76\%$ of the participants correctly identified two instruments. Only $18\%$ of the participants underestimated by one instrument, $6\%$ overestimated by one instrument.''
This surprising outcome then triggered the idea to deepen research on  unison instrumental recordings as presented in Chapter~\ref{cha:known}.

\section{Speaker Count Estimation}%
\label{sec:count_experiment}

\marginpar{This section was previously published in~\cite{stoeter19} and is reprinted, with minor modifications, with permission.}

Humans are excellent in segregating one source from a mixture~\cite{bregman90} and tend to use this skill to perceptually segregate speakers before they can estimate a count, as highlighted, e.g.\ in~\cite{kawashima15}.
As shown in~\cite{kashino96, kawashima15} with extensive experiments using Japanese speech samples, humans are able to correctly estimate up to three simultaneously active speakers without using any spatial cues.
In this experiment, we reproduced the experiments conducted in~\cite{kawashima15, kashino96} using stimuli of English speakers.
Designed to address source separation research, we also modified the question to ask participants for ``the maximum number of concurrent speakers'' in a short excerpt of speech.

\subsection{Stimuli}
To date, many available speech datasets contain recordings where only a single speaker is active.
Datasets that include overlapped speech segments either lack accurate annotations because the annotation of speech onsets and offsets in mixtures is cumbersome for humans or lack a controlled auditory environment such as in TV/broadcasting scenarios~\cite{Gravier12}.
Since a realistic dataset of fully overlapped speakers is not available, we chose to generate synthetic mixtures.
We recognize that in a simulated ``cocktail-party'' environment, mixtures lack the conversational aspect of human communication but provide a controlled environment which helps to understand how a DNN solves the count estimation problem.
As we aim for a speaker independent solution, we selected a speech corpus with preference to a high number of different speakers instead of the number of utterances, thus increasing the number of unique mixtures.
We selected \emph{LibriSpeech clean-360}~\cite{panayotov15} which includes 363 hours of clean speech of English utterances from 921 speakers (439 female and 482 male speakers) sampled at 16 kHz.
\par
To compute the maximum number concurrent speakers \(\cardinality\), annotation of the activity of each individual speaker is required.
Even though many corpora come with word and phonemes annotation, they often are not consistent across different corpora.
We, therefore, generated annotations based on a voice activity detection algorithm (VAD). As we rely on a robust VAD estimate, we found the implementation from the \emph{Chromium Web Browser} as part of the WebRTC Standard\footnote{WebRTC 1.0: Real-time Communication Between Browsers W3C Editor's Draft 05 June 2017} to yield good results.
\par
To generate a single example, a tuple of a speech mixture and its ground truth speaker count \(\cardinality \), we draw a unique set of \(\cardinality \) speakers from the corpus.
For each of the speakers, we then select a random utterance, resampled to 16 kHz sampling rate and apply VAD.\@
The VAD method was configured using default parameters using a hop size of 10~ms.
Further, the VAD estimate was used to remove silence from the beginning and the end of an utterance recording.
In the next step, more utterances from the same speaker are drawn from the corpus until the desired duration is reached.
We removed silence in the beginning and end of each utterance to increase the overlap within one segment.
Both, the audio recording and the VAD annotation of each utterance is concatenated.
The procedure is repeated for all speakers such that \(\cardinality \) time domain signals are created.
Signals are linearly mixed and peak normalized to avoid clipping.
The ground truth output \(\cardinality \) for each sample is then computed from the VAD matrix using the maximum of the sum over all speakers.
\par
In fact, our method to generate synthetic samples results in an average overlap for \(k=2\) of 85\% and for \(k=10\) of 55\% (based on 5s segments).
This procedure is similar to~\cite{mesaros17} used to label the data.
The Dataset is available for download~\cite{oss_libricount}.

\subsection{Experimental Setup}

\begin{figure}[htb]
    \centering
        \includegraphics[width=0.9\textwidth]{Chapters/07_Analysis_Experiment/figures/experiment_ui.png}
    \caption{Speaker count estimation experiment user interface.}
    \label{fig:user_interface_speaker}
\end{figure}

We conducted a study using the simulated data from the \emph{LIBRI Count Dataset} as mentioned in the previous subsection.
In turn, we randomly selected 10 samples for each \(k \in {1, \ldots, 10}\), resulting in 100 mixtures of 5~seconds duration each.
The stimuli were presented in random order using a custom web-based interface (depicted in Figure~\ref{fig:user_interface_speaker}) connected to a database API that saved the anonymized count responses and additional information about the participant session such as the response time.
The experiment was done using \emph{between-group design}, where one group (blind experiment) did not get any prior information about the maximum number of speakers in the test set (similar to~\cite{kawashima15}).
However, the maximum number of speakers was revealed to the other group (informed experiment).
Further, none of the participants received any feedback about the error made during the trials.
The participants were able to pause and resume the experiment at any time to reduce fatigue.
Similarly to~\cite{kawashima15}, lab-based experiments were conducted with ten participants for each group (\(n=20\)) using a custom designed web-based software.
None of the participants were native English speakers.
The experiment and its results from all participants is made available through~\cite{oss_countit}.
A simplified version of the experiment is made available through a web application\footnote{\url{https://denumerate.app}}.

\subsection{Experiment Results}
To reveal over and underestimation errors, we decided to report the average response for each group of \(k\).
As a reference, we also included the average results from~\cite[Experiment 1, 5~seconds duration]{kawashima15} which shows similar (with slightly higher error probability) results compared to our blind experiment.
Also, in~\cite{kawashima15} the maximum number of speakers to test was six whereas we evaluated stimuli with up to ten speakers.
The results of our lab-based experiments are shown in Figure~\ref{fig:experimentA} and Figure~\ref{fig:experimentB}.
Results indicate that underestimation becomes apparent for \(k > 3\).
First and foremost, we can confirm the ``one-two-three-many'' paradigm on our experiment with English utterances.
When we asked participants about the strategy they pursued, many reported that with more than three speakers it is not possible to identify (and count) the speakers but rather compare the \emph{density} of the speech to that of 1-3 speakers.
For higher speaker counts, participants reported that the phoneme density was a relevant cue that allowed them to extrapolate a source count estimate.
Interestingly, our results of the informed experiment reveal that they performed significantly better than those that participated blindly.
This is especially obvious for six and more speakers where the informed group performed better by more than one speaker in mean absolute deviation.

\begin{figure}[t!]
    \centering
    \begin{adjustbox}{width=0.7\columnwidth}
      \input{Chapters/07_Analysis_Experiment/figures/responses}
    \end{adjustbox}
    \caption{Average responses in comparison to the results of \emph{Kawashima}~\cite{kawashima15}. Error bars show 95\% confidence intervals (not available for~\cite{kawashima15}). \textsuperscript{\textregistered}2019 IEEE.}%
    \label{fig:experimentA}
 \end{figure}

\begin{figure}[t!]
   \centering
   \begin{adjustbox}{width=0.7\columnwidth}
     \input{Chapters/07_Analysis_Experiment/figures/success}
   \end{adjustbox}
   \caption{Mean probability of correct responses in comparison to the results of \emph{Kawashima}~\cite{kawashima15}. Error bars show 95\% confidence intervals (not available for~\cite{kawashima15}). \textsuperscript{\textregistered}2019 IEEE.}%
   \label{fig:experimentB}
\end{figure}


\section{Summary and Discussion}

% [X] A summary of findings and limitations
% [ ] Practical applications/implications
% [X] Recommendations for further research

In this chapter, we presented two experiments that we conducted to get a better understanding of the human ability to estimate the number of sources in overlapped mixtures.
\par
First, we showed that estimating the number of instruments in music mixtures is a challenging task for humans.
We presented the results of a controlled experiment with 62 participants, listening to stimuli of inhomogeneous timbres.
Our experiment indicated that the upper limit is reached with more than three instruments, related to an earlier experiment~\cite{huron89} focused on voices instead of instruments.
In Chapter~\ref{cha:fundamentals}, two different questions were introduced how to frame the count estimation problem.
In this experiment, we explicitly used an open question of ``How many instruments can you hear'' to gather more knowledge into the strategies being applied by the participants.
Another reason was that the stimuli durations were too long to ask for the maximum number of concurrent stimuli, thus promoting ``counting by detection'' as a good strategy to approach the problem.\par
By comparing musicians to non-musicians, we showed that there is a significant difference in performance for count estimation, confirming similar findings in other auditory tasks~\cite{kishon01}.
Particularly, we found a that this gap is most prominent for stimuli of three and four instruments.
Furthermore, for all stimuli (ranging from one to six instruments) we revealed that musicians performed about 20\% better than non-musicians, hence revealing a ``gap of one instrument'' in mean absolute error.
\par
We then repeated this experiment in an open, uncontrolled environment with more than 1000 participants.
To our knowledge, this was the first larger crowdsourced auditory experiment within the Signal Processing or MIR community.
In comparison to audio quality experiments like MUSHRA~\cite{recommendation2001MUSHRA}, we can, therefore, conclude that count estimation tasks are suitable for highly scalable crowd sourced listening experiments.
\par
In our second experiment, we reproduced an earlier study presented in~\cite{kawashima15} to estimate the maximum number of concurrent speakers in short audio mixtures, simulating a ``cocktail-party'' environment.
Our experiment went a step further with respect to the maximum amount of speakers to be estimated (up two \(k=10\)).
The results indicated almost no person was able to correctly estimate up to ten speakers.
However, we also observed that even for more than seven speakers the mean response did further (but not linearly) increase.
This indicates that humans are possibly interpolating some sort of a speech density to make up their decision.
\par
We conclude that all of our count experiments share the same results: A) humans are unable to correctly estimate more than four sources and B), underestimation is the main cause of error.
Also, we revealed that source count estimation studies can be easily be participated by non-professional listeners.