\begin{abstract}
Estimating the maximum number of concurrent speakers from single-channel mixtures is a challenging problem and an essential first step to address various audio-based tasks such as blind source separation, speaker diarization, and audio surveillance.
% Building upon powerful machine learning methodology and the possibility to generate large amounts of learning data, Deep Neural Network (DNN) architectures are well suited to directly estimate speaker counts.
We propose a unifying probabilistic paradigm, where deep neural network architectures are used to infer output posterior distributions.
These probabilities are in turn processed to yield discrete point estimates.
Designing such architectures mostly involves two important and complementary aspects that we investigate and discuss.
First, we study how recent advances in deep architectures may be exploited for the task of speaker count estimation.
In particular, we show that convolutional recurrent neural networks clearly outperform recurrent networks used in a previous study when adequate input features are used.
Even for short segments of speech mixtures, we can reliably estimate up to five speakers, with a significantly lower error than other methods.
Second, through comprehensive evaluation, we compare the best-performing method to several baselines, as well as the influence of gain variations, different data sets, and reverberation.
The output of our proposed method is compared to human performance.
Finally, we give insights into the strategy used by our proposed method.
\end{abstract}
