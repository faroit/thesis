\hypertarget{highly-overlapped-signals}{%
\chapter{Highly Overlapped Signals}\label{cha:highly-overlapped-signals}}

In the previous chapter, I introduced basic signal processing fundamentals in the context of audio mixtures.
In this chapter I want to focus on scenarios of highly overlapped signals.
In order to make an attempt to formalize the \emph{level of overlap}, I start with a recap of time-frequency representations as introduced in Section~\ref{sub:time-frequency-representation} and how they are typically applied to address the source separation problem.

\hypertarget{separability-of-mixtures}{%
\section{Separability of Mixtures}\label{separability-of-mixtures}}

TF representations have obvious benefits such as the improved interpretability due to its ``image-like'' two-dimensional properties.
More importantly, however, such a representation allows to separate mixtures of speech and musical instruments.
The reason for this is that these mixtures may be fully overlapped in the time domain but are less overlapped in the frequency domain~\cite{rickard02, giannoulis11, rafii}.
% still minor relation to rafii, but seems ok
In turn, a TF representation allows to apply filtering in a way that sufficiently extracts all targets from the mixture.
Furthermore, it allows for simple reconstruction of the original waveform and provides a good trade-off between computational complexity and separation quality.
\par
Due to these reasons, many source separation methods focus on extracting individual sources by modeling their respective target in the time-frequency domain.
% check
It is a reasonable approach where it is usually assumed that the time-frequency-domain (STFT) provides sufficient level of separability (e.g.~by given a perfect mask).
The actual extraction or filtering is done by synthesizing the magnitude estimate of the model and applying the originals mixture phase.
\par
In practice, the ability to extract a source from a mixture, depends on the amount of overlap between sources.
Without any overlap, separation is obviously not necessary, and a small amount of overlap can be tolerated to still sufficiently extract the sources.
However, if sources are fully overlapped in both, time and frequency, a separation in the TF domain is hardly possible.
A metric that is often used for the purpose of evaluation is called \emph{separability} and was found by Rickard in~\cite{rickard02} as a useful metric for both, speech and music~\cite{giannoulis11} signals.
\par
% check
In linear mixtures, separability is defined as \emph{a measure that indicates the percentage of time-frequency bins of a source is disjoint from those of interfering sources}.
If \(M\) is the ideal binary mask for a given target \(S\) and it's interfering
magnitude model \(Y\), \(WDO\) is defined as:

\begin{equation}
    PSR_{M} = \frac{\|M \cdot S_{k}\|^{2}}{\|S_{k}\|^{2}}
\end{equation}

\begin{equation}
    SIR_{M}=\frac{\|M \cdot S_{k}\|^{2}}{\|M \cdot Y_{k}\|^{2}}
\end{equation}

\begin{equation}
    WDO_{M} = PSR_{M} - \frac{PSR_{M}}{SIR_{M}}
\end{equation}

%Ideal Ratio Mask:
%\(\hat{\mathbf{x}}=\frac{\mathbf{v_j}}{\sum_{j'=1}^{J}\mathbf{v}_j'}\mathbf{x}\)

A \(WDO\) of one means the sources are perfectly disjoint, hence no overlap.
A \(WDO\) zero means can be interpreted as sources being fully overlapped.
An extensive study using this W-disjoint orthogonality metric (WDO) is given in~\cite{rickard02}.
\par
Research on mixtures is often focused on specific, application-driven scenarios to analyze and separate mixtures.
To give an example, I want to focus on two scenarios that are very popular among research, each with a large amount of contributions:

\begin{description}
  \item[Cocktail Party] where multiple speakers are speaking concurrently, it results in an overlap of speech signals in both time an frequency. 
  \item[Vocals and Accompaniment] are usually present at the same time in professionally produce music. 
  Often, the main assumption is that vocals are more sparse, whereas the accompaniment is more stationary.
\end{description}

Now, for both scenarios, the actual overlap is dependent on additional parameters like the the number of sources, the class of source or the fundamental frequency.
For instance, the overlap in a cocktail party of two speakers is smaller than with ten concurrent speakers speaking.
Also the overlap between male and female or brass and string instruments is smaller than with two instruments of the same class. 
And if two instrumental notes share the same fundamental frequency (playing in \emph{unison}), the sources are almost completely overlapped.
\par
\begin{figure*}[h]
\centering
\subcaptionbox[Speech]{Speech}%
[1\textwidth]{\includegraphics[width=0.8\textwidth]{gfx/dominance_map_speakers.pdf}}%
\hspace{0.2\textwidth} % seperation
\subcaptionbox[Vocal/Accompaniment]{Vocal/Accompaniment}
[1\textwidth]{\includegraphics[width=0.8\textwidth]{gfx/dominance_map_vocacc.pdf}}%
\hspace{0.2\textwidth} % seperation
\subcaptionbox[Speech]{Unsison}
[1\textwidth]{\includegraphics[width=0.8\textwidth]{gfx/dominance_map_unison.pdf}}%
\caption{Predominant source activity, showing the predominant source for each time frequency entry. Computed using binary masks of each source entry.}
\label{fig:dominance}
\end{figure*}

\begin{figure*}[h]
\centering
\subcaptionbox[Speech]{Speech}%
[1\textwidth]{\includegraphics[width=0.8\textwidth]{gfx/count_map_speakers.pdf}}%
\hspace{0.2\textwidth} % seperation
\subcaptionbox[Vocal/Accompaniment]{Vocal/Accompaniment}
[1\textwidth]{\includegraphics[width=0.8\textwidth]{gfx/count_map_vocacc.pdf}}%
\hspace{0.2\textwidth} % seperation
\subcaptionbox[Unison]{Unison Instruments}
[1\textwidth]{\includegraphics[width=0.8\textwidth]{gfx/count_map_unison.pdf}}%
\caption{Source Count Activity showing the number of sources $k$ for each time frequency entry. Computed using binary masks of each source entry.}
\label{fig:count}
\end{figure*}

To illustrate this, I depict the different scenarios in Figures~\ref{fig:dominance} and~\ref{fig:count}.
The Figure~\ref{fig:dominance} shows the number of TF entries that predominantly belong to either one of the two source classes.
Figure~\ref{fig:count} shows the number of active sources of each TF entry for the same audio signals.
From these figures, one can see that the overlap of a typical speech mixture is comparable to a music recording where the task is to separate vocals and accompaniment.
If we now compare this to the scenario where sources are fully overlapped as in the unison scenario, almost all TF bins are overlapped and separation would hardly be possible.
\par
While this is an extreme scenario, it still provides a useful example where typical assumptions are violated and it would facilitate the demand to develop new methods that do not rely so much on these long standing assumptions.
In fact, a na√Øve method can be developed by just observing closely the spectrograms of Figure~\ref{fig:count}.
We see that the slow spectro-temporal modulations caused by the vibratos is one of the aspects where the two sources differ significantly.
So here, the classical STFT does not sufficiently offer enough separability and representations like the \emph{modulation spectrogram}, presented by~\cite{greenberg96}, allow to separate the two sources in a \emph{modulation domain}.
Details about this approach are discussed in Chapter 5.
Now I want to focus on these ``modulational'' aspects on mixtures and their applications in the next section.

\hypertarget{exploiting-slow-modulations}{%
\section{Exploiting Slow Modulations}\label{exploiting-slow-modulations}}

Tempo-spectral modulations occur both in speech and music signals as detailed in Section~\ref{sub:time-variant-audio-signals}).
Exploiting modulations is natural for humans: early research from Zwicker in 1952 focused on the human ability to detect amplitude modulations~\cite{zwicker52}. 
Later, it was shown by Bregman, McAdams and Fastl in~\cite{mcadams89, bregman90, fastl90} that humans use amplitude modulations to group sources; this concept was called \emph{Common Amplitude Modulation} (CAM).
CAM exploits the fact that harmonics that share the same amplitude modulation across the bins, are perceived as \emph{integrated} as opposed to~\emph{segregated}.
Further, it was shown in~\cite{bacon89} that the ability of detecting amplitude modulations can be integrated into auditory models.
It was then found by Dau in~\cite{dau99}, humans are especially sensitive at slow modulations:

\begin{quote}
``Slow modulations are associated with the perception of rhythm. Samples of running speech, for example, show distributions of modulation frequencies with peaks around 3-4 Hz, approximately corresponding to the sequence rate of syllables~\cite{plomp83}. Results from physiological studies have shown that, at least in mammals, the auditory cortex seems to be limited in its ability to follow fast temporal changes.''
\end{quote}

Dau, in fact, proposed a model that mimics the humans ability to detect modulation patterns and pointed out applications to improve hearing of hearing-impaired listener or speech intelligibility.
\par
Auditory science shows that modulations are important for humans, possible research questions for the scope of this thesis are:

\begin{itemize}
  \item Can modulations be automatically detected or extracted from highly overlapped signals?
  \item If modulations are known, can they be exploited to improve a task such as source separation?
  \item Can modulations still be utilized when modulation function and its cause of modulation is unknown?
\end{itemize}

% depending on the carrier frequency and the modulation frequency, humans describe modulations differently as fluctuation, roughness, or residue pitch (See Figure 2 in~\cite{joris04}).
Previously, research aimed to address these questions for various tasks of processing and analysis. 
In the following, I will give an overview of existing work focussed on analysis and separation of modulated sounds.

\subsection{Analysis}

Modulations play an important role for various audio analysis tasks.
%lets start with speech
In speech techniques to utilize modulation patterns improved applications such as speech discrimination~\cite{mesgarani04} or extract spatial acoustic signatures from mixtures~\cite{sukittanon06}.
One way of analyzing amplitude modulations is to use a modulation spectrogram~\cite{greenberg97} which is a frequency-frequency representation of a time domain input signal.
A complete signal representation can be archived by a modulation tensor which holds the modulation spectrograms for each time frame.
Interestingly, Greenberg assumed that ``the energy in the modulation spectrum may be derived from syllabic segmentation'' and that ``the preservation of the portion of the modulation spectrum between 2 and 10 Hz''.
Following this, it was later proved that the detection of modulations can improve speech intelligibility~\cite{elhilali03} or automatic speech recognition~\cite{kingsbury98}.
\par
% go to music
Amplitude modulations were also proposed for music tasks. 
Work by Scheirer in~\cite{scheirer99}, proposed a method that operates by utilizing common modulation among groups of frequency sub-bands in the autocorrelogram domain.
In music, predominantly caused by vibrato, amplitude modulations are considered to be that important and the reader is referred to~\cite{muller11} for further details.
\par
A common way to explicitly analyze frequency variations is to first analyze the fundamental frequency and then track the fundamental frequency over time. 
An overview of various techniques are summarized in~\cite{driedger16}.
The authors of this paper also proposed a novel method to directly estimate the parameters of potential frequency modulations in the time-frequency domain by matching sinusoidal templates.
\par
For time-varying frequency modulations, the modulation spectrogram is not an effective, as it would only be able to track the modulation rate due the side-lobes of the modulation.
Disch and Edler proposed to decompose an audio signal into band pass signals and each of them parametrically modeled by a sinusoidal carrier and its amplitude and frequency modulation~\cite{disch09}.

\subsection{Separation}

As described in Chapter 2, modulations are used by humans to group and segregate sounds. Viste et al. describes the impact of modulation in~\cite{viste03} as:

\begin{quote}
``harmonic relation, the common onset, offset, amplitude modulation (AM), and frequency modulation (FM).These are all important cues for grouping.''
\end{quote}

It is therefore not surprising that a number of methods exist, that utilize spectro-temporal modulations to separate mixtures.
% from zafar!
Wang proposed instantaneous and frequency-warped techniques for signal parameterization and source separation, with application to voice separation in music~\cite{wang94,wang95}.
He introduced a frequency-locked loop algorithm which uses multiple harmonically constrained trackers.
He computed the estimated fundamental frequency from a maximum-likelihood weighting of the tracking estimates. He was then able to estimate harmonic signals such as voices from complex mixtures.\\
% from zafar
The concept of \emph{common amplitude modulation} by~\cite{bregman, wang06} which exploits that amplitude envelopes of different harmonics of the same source tend to be similar.
Modeling the common amplitude modulation to separate mixtures has already been done in~\cite{li07, li09} and \cite{cano14}, which additionally included common amplitude modulation characteristics in the separation scheme.
% from zafar
Wolf et al. proposed an approach using rigid motion segmentation, with application to singing voice separation \cite{wolf14,wolf16}. They introduced harmonic template models with amplitude and pitch modulations defined by a velocity vector. They applied a wavelet transform \cite{anden14} on the harmonic template models to build an audio image where the amplitude and pitch dynamics can be separated through the velocity vector. They then derived a velocity equation, similar to the optical flow velocity equation used in images \cite{bernard01}, to segment velocity components. Finally, they identified the harmonic templates which model different sources in the mixture and separated them by approximating the velocity field over the corresponding harmonic template models.\\
% from zafar
Yen et al. proposed an approach using spectro-temporal modulation features \cite{yen14,yen15}. They decomposed a mixture using a two-stage auditory model which consists of a cochlear module \cite{chi05} and cortical module \cite{chi99}. They then extracted spectro-temporal modulation features from the TF units and clustered the TF units into harmonic, percussive, and vocal components using the EM algorithm and resynthesized the estimated signals.\\
% from zafar
% double check this!
Virtanen made us of sinusoidal modeling \cite{virtanen00} to model and separate sources with tempo spectral modulation like vibrato.
% TODO: more cites for source filter model
In another vein, the source-filter model was deployed to separation sources~\cite{hennequin10}.
An advantage of the source-filter model approach is indeed that one can dissociate the pitched content of the signal, embodied by the position of its harmonics, from its TF envelope which describes where the energy of the sound lies. In the case of vocals, it yields the ability to distinguish between the actual note being sung (pitch content) and the phoneme being uttered (mouth and vocal tract configuration), respectively. 
One key feature of vocals is they typically exhibit great variability in fundamental frequency over time. They can also exhibit larger \textit{vibratos} (fundamental frequency modulations) and \textit{tremolos} (amplitude modulations) in comparison to other instruments.

% TODO: add this to chapter commonfate: ~\cite{creager16} VibratoNTF uses DOA 

\section{Summary}

Modulations play an important role in audio signal analysis. 
However past research was mainly focused on the analysis of single notes and not on overlapped sounds.
It is therefore to be investigated if scenarios with severe overlap can utilize modulations as well.
This comes with a number of challenges: Firstly, parameterization of modulation characteristics of a single source is difficult, when only the mixture is observed. 
For example, it is known~\cite{salamon13} that the extraction of the fundamental frequency in a mixture is difficult. 
The reason is that crossing partials is a big problem for sinusoidal modeling~\cite{viste03}. 
And even if the tracking would work perfectly, evaluation of robustness and accuracy is often hardly possible when the reference data is only annotated with human precision.
Secondly, representations like modulation spectrogram only cover amplitude modulations, how can modulation patterns of generic nature (AM/FM, timbre modulation) tracked.
Highly overlapped signals require suitable representations that have a better separability than classical STFT.
Improving the separability, however, may not necessarily lead to better separation results when representations are not easily invertible.
% TODO: move to 5 or 6
% Another problem is that approaches that exploit modulations for source separation were mostly focussed on known modulations in contrast to unsupervised methods.
% Furthermore, with the recent success of deep learning based methods, it to be seen, if learning based methods can utilize modulations from data.
\par
In the next two chapters, I will introduce methods that utilize the modulations of sources through either prior knowledge (known) or by operating blindly (unknown).
In any case, this differentiation does only affect how modulations are incorporated --- the underlying method is based on both, supervised and unsupervised methods.