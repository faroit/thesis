\hypertarget{fundamentals}{%
\chapter{Fundamentals}\label{fundamentals}}

The main focus of this thesis is the \emph{analysis} and \emph{processing} of sound recordings of music or speech, commonly referred to as \emph{audio signals}.
In this chapter, I introduce basic concepts of digital audio signals which are relevant to follow the remaining chapters.

\hypertarget{Fundamentals of Overlapped Sounds}{%
\section{Fundamentals of Overlapped Sounds}\label{specifics-of-audio-signals}}

When a sound wave travels through a medium like air it can be captured using a microphone by measuring the local pressure deviation over time.
Such a signal ca be written as a function \(x(t)\) continuous in both, time \(t \in \sR\) and the amplitude \(x(t) \in \sR\).
An \emph{audio signal} can simply be introduced as a signal that is meant to be perceived by the human auditory system --- through our ears.
Therefore, one could observe specifics properties of audio signals, consistent to the limitations of the human hearing system, for example in dynamics/loudness as well as a limited signal bandwidth.
In fact, many other signals exist with similar characteristics such as signals from finance, geophysics, meteorology or medical data.
The result is that often audio research is inspired by applications of other fields of signal processing and vice versa.

\hypertarget{digital-representations-of-audio-signals}{%
\subsection{Digital Representations of Audio
Signals}\label{digital-representations-of-audio-signals}}

To store, analyze or process audio signals, as of today, digital representations are used to leverage the processing capabilities of modern computing devices.
A digital audio signal can be obtained from an analog signal using analog-to-digital/digital-to-analog converters (ADC/DAC) which can be found in almost any every-day device such as laptops and smart-phones.
In short, this includes to steps, First, the continuous time signal \(x(t)\) is converted to a discrete time series, where one sample\footnote{Please note, that the use of the word \emph{sample} will have different meanings in the context of machine learning, where a sample is usually a instance of a signal instead of a single time step.} \(x(n)\) using equidistant steps \(T\) using \emph{sampling} and Second, the amplitude values are \emph{quantized} resulting in vector \(\vx \in \sZ\), represented as a one dimensional time series of amplitudes.
An important parameter in the process of digitization is the sample rate \(F_s = 1/T\) where \(T\) is the sampling period.
Often, \(F_s\) has a significant effect on the quality of audio signals.
And, it is the objective of a real world audio system to not introduce perceptible degradation in audio quality when a digital signal is reproduced over headphones or loudspeakers.
Due to the Nyquist-Shannon sampling theorem the sample rate needs to be at least twice the band width of the analogue signal.
Since the hearing range of normal listening humans is
20~\si{\hertz} - 20~\si{\kilo\hertz}~\cite{fastl90, moore89}, typically for audio signals, 
sample rates of 44100~\si{\hertz} are chosen to facilitate the full human hearing range.
However, for many applications, a lower sampling rate is sufficient, e.g. in speech communication where intelligibility often is more important than quality.
For further details we refer the reader to audio signal processing basics such as~Chapter 1 in~\cite{proakis96} or Chapter 2 in~\cite{Mueller15}.

\hypertarget{time-frequency-representation}{%
\subsection{Time-Frequency Representation}\label{time-frequency-representation}}

Real world sounds such as speech and music include periodicity.
Therefore, sounds are often analyzed in the frequency domain as a \emph{spectrum} to reduce redundancy and improve the computational efficiency of the signal processing.
This is commonly achieved trough the use of discrete Fourier transform (DFT) and its fast FFT implementation~\cite{cooley65}. 
For details, the reader is referred to~Chapter 4 of~\cite{proakis96}.
At the same time, spectral representations relates to our human auditory system~\cite{zwicker13, moore89, bregman90} allowing us to process sounds the way we perceive them.
\par
The periodicity of real world sounds, as mentioned above, usually only holds for short time scales of several milliseconds, often referred to as ``quasi-stationarity''.
Instead, sounds are non-stationary over the course of several seconds and it therefore is useful to analyze and process short-time spectra, computed in an overlapped fashion, resulting in a \emph{time-frequency} (TF) representation.
Simply put, a TF representation encodes the time-varying \textit{spectra} into a matrix \(\mX(k\) with frequencies \(k\) and time frames \(n\).
For signals with more than one channel, the representation is extended to yield a three-dimensional tensor, where \(c\) includes the channel.
\par
When sounds are processed in the time-frequency domain the transformation needs to be inverted, hence many researchers chose a representation that allows to easily reconstruct the signal.
The short-time Fourier transform (STFT) is the most commonly used TF representation~\cite{mcaulay86}.
STFT matrices \(\mX \in \sC^{n \times k \times c}\) are complex meaning that they include the phase information along with and the magnitude which represents the amplitude of that signal.
Often the magnitude of the STFT is referred to as the~\emph{spectrogram}.

\subsection{Fundamental Frequency, Pitch and Harmonicity}
% TODO: Add STFT plot from Cello Dataset, showing f0, overtones vibrato extend etc.
% \cite{stoeter15acm}, to be downloaded in \cite{oss}

% from zafar
Speech and music signals contain is characterized by its periodicity.
And it is this property that humans perceive as \emph{pitched}.
\emph{Pitch} is defined by Klapuri~\cite{klapuri06book} as ``a perceptual attribute which allows the ordering of sounds on a frequency-related scale extending from high to low''.
It is important to note that the term \emph{pitch} is a subjective measure.
The objective equivalent is referred to as the fundamental frequency (\(F_0\))\footnote{Pitch and $F_{0}$ are often used synonymously in audio research. Even though this is incorrect I will sometimes refer other work where pitch is instead of $F_{0}$.}.
\(F_{0}\) is can be defined as the lowest frequency/partial of an harmonic signal.
All frequencies together formed by the integer multiples of the fundamental frequency are named  \textit{harmonics}~\cite{schenker54}.
When the fundamental frequency changes, the frequencies of these harmonics changes accordingly.
This results in the typical comb-like spectrograms of harmonic signals.
For a a detailed overview into the research field of pitch and \(F_{0}\), the reader is referred to~\cite{decheveigne05, klapuri06book, salamon13}.
\par
An estimate of the fundamental frequency of a signal is required in various applications of audio and speech signal processing.
Some scenarios are targeted to extract the fundamental frequency of the predominant source~\cite{salamon12} in a mixture of other sources.
In other applications, algorithms are used to extract fundamental frequencies of multiple sources simultaneously present in a signal~\cite{klapuri03}.
However, the most common scenario in many works is to extract the fundamental frequency of a monophonic and harmonic audio signal containing speech or music~\cite{talkin95, boersma02, decheveigne02, resch07, tidhar10, christensen07, stoeter15icassp}.

\subsection{Time-Variant Audio Signals}

Audio signals are considered to be stationary or time-invariant when certain properties such as the amplitude or the fundamental frequency of the signal do not change over time.
A stationary sound can be modified to turn it into time-invariant sound. 
In signal processing this is referred to as modulations and is, in fact, of paramount importance as it has enabled technologies such as radio transmission~\cite{shannon48}.
Modulations can be defined as time-variant functions that modulate a target parameter of signal over time.
In the case of audio signals, often both, the modulating function (modulator) and the signal being modulated are are periodic.
Signal modulations are often created intentionally but also occur naturally in many real-world audio signals such as speech.
In the following, I will present a typical audio modulation categories and their cause, underlining the importance of them.

\paragraph{Audio Signal Communication}

The transmission of an audio signal using a modulator/demodulator (modem) system may be one of the most important applications for audio modulations. 
The principle is used the broadcasting of radio using a high frequency carrier signal that is modulated by an audio signal to be transmitted, which is assumed to be of slower frequency than the carrier signal.
The modulator is targeted to vary the amplitude or the frequency of the carrier signal.
If we imagine a sinusoidal carrier signal $x(t) = \cos \omega_c t$, \emph{amplitude modulation} (AM) is applying by a modulation function $a(t)$ so that:

\begin{equation*}
    s_{AM}(t) = a(t) \cos \left( \omega_{c} t\right).    
\end{equation*}

In comparison to AM, Frequency modulation (FM) varies the frequency of the carrier, so that:

\begin{equation*}
    s_{FM}(t) = A \exp \left\{ \omega_ { 0 } t + j m \sin \left( \omega_ { m } t \right) \right\}
\end{equation*}

where A is the amplitude, $\omega_0$ is the carrier frequency, $\omega_m(\theta)$ is the instantaneous modulation frequency, and $m$ is the modulation index $m = \frac{\Delta \omega_0} { \omega_{m} }$.

It is well known that the Fourier spectrum of $z_{FM}(t)$ is quite difficult due to its non-linearity and involves computations using the Bessel function.
For applications where the modulation frequencies that are a lot smaller than the carrier frequency, the spectrum that is caused by frequency modulations can be similar to those of amplitude modulations.
The total bandwidth in this scenario is approximately $2\Delta \omega_0$, as found by Carson in~\cite{carson22}.

\paragraph{Modulations in Music}

Both, frequency and amplitude modulations are a recurrent phenomenon in music.
Here, the modulations are created by a conscious physical manipulation of a musician to make a sound more pleasant to the listener.
In traditional instruments, modulations are often referred to as \emph{vibrato}, which is defined by~\cite{seashore31} as

\begin{quote}
...a periodic pulsation, generally involving pitch, intensity, and timbre, which produces a pleasing flexibility, mellowness and richness of tone.
\end{quote}

Vibrato is an essential playing style for string instruments like a violin. 
For these instruments, that are usually plucked or bowed, the strings are the primary source of excitation that is modulated in frequency by the players finger on a fretboard (See~\cite{macleod06}).
Modulations can also be realized with woodwind and brass instruments, however, instead of the excitation signal, the resonator is modulated which allows for periodic modulations of the timbre.
Many musicians use similar modulation frequencies (also called \emph{rate}) to perform a vibrato.
It varies between musicians and is usually in the range of 4-8\si{\hertz}.
A detailed overview of the different musical instruments and their modulation characteristics can be found in~\cite{fletcher01}.
\par
Real instruments are not capable of purely amplitude modulated sounds (\emph{tremolo}). 
Today, many instruments are electric or are attached to electronic effects where such modulations can be applied using digital oder analog signal processing.
For example, popular electric pianos like the ``Fender Rhodes'' can generate include an optional tremolo effect\footnote{Even though it was labeled as \emph{vibrato}.}.
In its most pure form, synthesizers like~\cite{pinch09, buchla05} allow to modulate almost any parameters of a sound using low frequency oscillators (LFOs) or envelopes that produce sinusoidal, square or triangle functions of any rate.
In fact, one of the most important sound synthesis methods --- FM Synthesis --- became popular in the early days of digital signal processing. 
Chowning~\cite{chowning73} found that the modulation of sinusoids using audio rate modulators, provides a computationally efficient way of producing very complex sounds that allow to mimic piano sounds with just four sinusoidal modulators.

% ## Singing Voice
% rate, extend

% vocal system is a pulsation of subglottal pressure. Itself some kind of AM
% Vocal tract shape rhythmically moving

% \cite{desain99}

% \cite{sundberg94} Very extensive study on vocal vibrato. of all different fazentten

% from \cite{sundberg94}
% > The main perceptual effect of the vibrato is dependent upon the frequency modulation and it is generally quite hard to focus one's attention on the amplitude fluctuations.

% ~\cite{sundberg}
% ```
% Vibrato rate, which is the frequency of modulation in Hz).
% Vibrato extent, the magnitude of said peak (in cents).
% ```

% General psychoacoustic theory: \cite{bregman}

% Fletcher defiens vibrato as ``...an oscillation of pitch, loudness or timbre of a musical tone...''~\cite{fletcher01}.

% * often modulations are combined.
% * there are two ways to get a modulation index.

% > sundberg: The vibrato tones were characterized by a fundamental frequency undulation at a rate of 5 to 7 undulations per second and an extent of about *1 semitone.

% - rate: 5-7 Hz
% - source: sinusoidal, triangular, trapezoidal
% - target: frequency, amplitude

% ## Modulations in Speech

% Speech 4 and 16 Hz 
% from Greenberg97

% \cite{fuellgrabe09}
% > It has been previously suggested that the broad peak at 4 Hz in the modulation spectrum corresponds to the average syllable rate [8].

% [8] => ~\cite{plomp83, Houtgast85}


% roughness and residue pitch can usually not considered as slow modulations but rather medium to fast modulations up to a few hundred Herz.

% - rate: 2-10 Hz
% - source: syllable durations, glottal pulse, 
% - target: vocal folds

% ## physiological

% \cite{zwicker52, plomp93, fastl90}
% Deep neural study to analyse what is the reason why the human auditory system can detect amplitude modulations so well~\cite{joris04}.

% The upper limit of modulation detection extends to 2.2 kHz

% <20Hz -> ~\cite{schreiner88}

% ~\cite{plomp93} showed that the cortex is capable of processing rhythm-like envelope fluctuations.

% ## Pathological

% * vocal tremor \cite{ramig87}
% * Parkinson shaking~\cite{botzel14}
% * Because it allows to...



% TODO: move this to fundamentals?
In this work we assume that we can separate overlapping partials of the sources based on differences in amplitude and/or frequency modulation, resulting in the following model for a signal with $P$ commonly modulated partials
\begin{equation}
  \begin{array}{l}
   x(n) = \displaystyle \sum_{p=1}^{P} \Big[\big(1 + a(n)\big) \\
   \hspace{3.5em}\displaystyle \cdot\sin \Big(2\pi f_{p,0}\big(n + \frac{1}{f_{1,0}} \sum_{m=m_0}^{n}{f(m)} \big) + \phi_{p,0} \Big)\Big] ,
  \end{array}
\end{equation}
where effectively the amplitude modulation is $a(n)$ and the frequency modulation of the first partial is $f(n)$.

\begin{table}[]
\scriptsize
\centering
\begin{adjustbox}{angle=90}
\begin{tabular}{@{}lllp{3.5cm}ll@{}}
\toprule
                          & Carrier Signal    & Modulation Function & Modulated Parameters        & Modulation rate & References\\ 
\midrule
AM/FM/PM                  & HF Sinusoid       & any audio signal    & amplitude, frequency, phase & up to 20~\si{\kilo\hertz}     & \cite{shannon48}        \\
String Vibrato & String Excitation & sinusoidal-like     & string length   & 5-7~\si{\hertz}          & \cite{fletcher01, macleod06}\\
Woodwind and Brass Vibrato & Resonator & sinusoidal-like & amplitude, timbre               & 4-8~\si{\hertz}          & \cite{fletcher01, gilbert05}\\
Modular Synthesizer       & any audio signal  & LFO                 & any parameter               & not limited     & \cite{buchla05, pinch09}\\ 
Electronic Organ with Tremolo  & instrument sound & doppler effect      & amplitude and frequency & 1~\si{\hertz}/7~\si{\hertz} & \cite{leslie49} \\
Singing Voice  & vocal &  sinusoidal/triangular &  amplitude and frequency & 5-8~\si{\hertz} & \cite{sundberg94} \\
Speech  & glottal pulse, language & not defined & amplitude, frequency, \newline phoneme duration, timbre& 4-10~\si{\hertz} & \cite{plomp83, fuellgrabe09}\\
Parkinsonian tremor  & muscle activity &  sinusoidal-like & amplitude & 5~\si{\hertz} & \cite{botzel14}\\
Auditory cortex  & nerve activity &  sinusoidal-like & amplitude & up to 20~\si{\hertz} & \cite{schreiner88}\\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Overview of modulations in audio signals and a selection of their respective properties.}%
\label{tab:modulations}
\end{table}

\hypertarget{sources-and-mixtures}{%
\section{Sources and Mixtures}\label{sources-and-mixtures}}

The definition of a sound source builds upon the underlying physical
phenomenon of the actual \emph{acoustical} emission of a sound and its
spatial position.
In the real world, however, single isolated sound sources are rare.
Instead we are often faced with multiple sources that render a called acoustical sound scene.
In this setting, one can define a \emph{set of sources} where the set is of arbitrary size.
Sets can include other sets, thus representing acoustical scenes of hierarchical structure.

When multiple sources are active at the same time the sound that reaches our ears or is recorded using a microphone is superimposed or \emph{mixed} into a single sound.
The \emph{mixture} is a mapping from a set of sources
\(\mathbf{s_j}\) to a target \(\mathbf{x}\).
There exist a variety of different mixing models that are utilized in literature and throughout this thesis.
Usually they are build upon several assumptions to constrain the scenario and model specific aspects of real world signals.
The most important assumption is that the mixing is linear so that the sum of all sources result to the mixture.
The non-linear case would be relevant for professionally produced music where audio effects are applied in post-processing.
While this might not represent all real world mixtures, it is usually a good approximation.
Another distinction is between instantaneous or convolutive mixtures.
For instantaneous mixtures, all sources are mixed using fixed mixing parameters \(a_j\).
This is the typical scenario when sources are mixed using a mixing console (pan-pot mix).
In \emph{convolutive} mixtures, each source \(\mathbf{s_j}\) is convolved with a filter response \(r_j\).
This scenario approximates a real world acoustic environment like a cocktail party where each speaker is then convoluted with a room impulse response.
Usually, the mixing process is assumed to be time-invariant but for a variety of signals such as EEG data or music live recording with moving sources it can also be time-variant.
In the remainder of this thesis, however, I will only consider the time-variant case.
I summarize the mathematical notation of different mixing models in Table~\ref{tab:mixing_models}.

\begin{table}[]
    \centering
\begin{longtable}[]{lll}
\toprule
\begin{minipage}[b]{0.26\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.42\columnwidth}\raggedright
Instantaneous\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\raggedright
Convolutive\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.26\columnwidth}\raggedright
Time-Invariant\strut
\end{minipage} & \begin{minipage}[t]{0.42\columnwidth}\raggedright
\(\mathbf{x}=\sum_{j=1}^{J}a_j\mathbf{s}_j\)\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
\(\mathbf{x} = \sum_{j=1}^{J}r_{j} \ast \mathbf{s}_j\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.26\columnwidth}\raggedright
Time-Variant\strut
\end{minipage} & \begin{minipage}[t]{0.42\columnwidth}\raggedright
\(\mathbf{x}=\sum_{j=1}^{J}a_j(n)\mathbf{s}_j\)\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
\(\mathbf{x} = \sum_{j=1}^{J}r_{j}(n) \ast \mathbf{s}_j\)\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}
    \caption{Overview of linear mixing models.}
    \label{tab:mixing_models}
\end{table}

% from zafar >> rewrite
The mixing process can equivalently be described in the TF domains as the {DFT} is a linear operator.
Therefore the summation of two signals in time domain extends to the frequency domain.
However, since we are normally dealing with the non-negative magnitude representation this is only an approximation~\cite{klapuri06}.

\paragraph{Specifics of Music Mixtures}
\label{par:specifics_of_music_mixtures}

In music it is important to understand that the process of mixing is of paramount importance to successfully model music.
This is because mixing sources is often considered as a creative task that involves recording engineers and tonmeisters.
In today's recording processes which involve digital mastering, professionally produced music consists of several intermediate mixing steps before the final mixture is produced:

\begin{description}
  \item[1) Microphone Recording:] in this step the analog sources are captured and D/A converted. Vocals and other acoustic instruments are recorded using one or multiple microphones.
  Electric instruments such as keyboards or synthesizers may be amplified and then directly digitized.
  \item[2) Raw Source Image:] the digital raw source signals are then grouped and mixed together into what is called a \emph{source image\footnote{Sometimes this is referred to as ``stem''.}}.
  This grouping is involves a creative process, hence it is usually done by a recording engineer.
  The source image is usually mixed to a specific number of output channels (often: stereo) even though the recording is done by a mono microphone (e.g. vocals) or multiple microphones (e.g. drums).
  In this stage a linear panning is added to spatially position either the sources or the image.
  \item[3) Mastered Source Image:] for each of the images an additional mastering step could be applied that may involves additional processing.
  At this stage, often artificial reverberation is added.
  \item[4) Raw Mix:] all source images are mixed for the intended target such as a stereo.
  This step is usually a linear sum of all images.
  \item[5: Mastered Mix:] again, an additional mastering is applied.
  Often this step involves non-linear processing such as dynamic range compression.
\end{description}

In each of these steps, one of the mixing models described in Table~\ref{tab:mixing_models} can be assumed which is why modeling music mixtures is considered to be a very challenging problem.
In fact, this emphasizes that the definition of a musical audio source cannot clearly be given as it is subjective and depends on the application and its context.
This is problematic for various applications that deal with modeling mixtures and their underlying sources.

\hypertarget{processing-and-analysis-of-mixtures}{%
\section{Tasks for processing and Analysis of Mixtures}\label{processing-and-analysis-of-mixtures}}

Speech is one of the most important signal types as it is fundamental for humans to enable communication.
When humans talk to each other, we are often not aware of the fact that we are listening to a mixture of several sources (e.g. from multiple talkers) or noise.
For many applications, some of the sources in these mixtures may not be desired~\cite{lorem}.
Often a specific talker is considered as the desired source used to carry the actual information whereas the noise or other speakers interfere with this signal.
The attenuation of undesired speakers when multiple concurrent speakers are active is well known as the ``cocktail party problem''~\cite{cherry53, haykin05}.
Surprisingly, it has been shown that humans are able to carry out this task for a desired source, even without eye contact and when not spatial cues can be used~\cite{bregman90}.
Therefore, for a long time, research is fascinated by the idea to create a machine to inverse the process of mixing  and to extract or separate the desired sources from its mixture.
This problem is called \emph{source separation}.
% It is similar to when we make a photograph of an object with many other objects visible in the same scene, sometimes occluding the object of interest.\\

\subsection{Source Separation}
The earliest work on audio source separation started in the mid 50s~~\cite{} on speech data and remains today is still a very active field of research with a large number of contributions.
It yields very specific challenges and opportunities and in the past a huge number of scientific contributions were made.
Source separation methods has both relevant applications for music and speech mixtures such as X, Y, Z. Often it enables other tasks such as A, B, C, D.
Due to both its importance and their large number of applications, source separation has been a popular topic in signal processing for decades.\\
% from zafar
While early works started with speech separation, the separation of musical sources is dating back to the 1970's.
They are mostly based on classic digital signal processing techniques, generally attempt to exploit the pitch information of the lead musical source and typically require some manual interaction~\cite{miller73, oppenheim68, oppenheim682}.
Due to the high number of contributions in this research field, it is not feasible to give an extensive overview on existing methods in the context of this thesis.
Often researchers do not deal with the general source separation problem but propose contributions to specific sub problems, usually targeted at a much more constraint scenario.
I therefore decided to present my key decisions that I made for the work in this thesis and I refer the readers to overview literature with respect to other scenarios when appropriate.

\subsubsection{Underdetermined vs. Overdetermined Separation}
As mentioned in Section~\ref{sources-and-mixtures}, generating sound mixtures is closely related to the process of the mixing taken place during recording (speech) or with the help of professional recording engineers (music).
One assumption that was not mentioned before, is the importance of the number sensors or microphones used to create the mixture.
A source separation problem is \emph{over-determined} when the number of sources is smaller than the number of sensors; \emph{determined} when they are equal.
For these two cases, a large number of method exist and in same cases a closed form solution is possible.
The reader is referred to~\cite{common10}, which is gives a detailed overview of these methods.\\
Many real world source separation problems, however, are under-determined and up to date for a large number of scenarios, the problem of separating sources is still very challenging.
In this thesis I only focus on methods that perform separation on underdetermined mixtures.
% Todo: add argumentation here

\subsubsection{Single Channel vs. Multichannel Separation}
% from zafar
Approaches based on panning information aim at exploiting stereo cues to identify and separate individual sources. Such approaches typically compare the left and right channels of a mixture in the TF domain to estimate the \textit{panning coefficients} of the sources and then generate TF masks to separate them. They generally assume that the one source has a fixed panning, which is often the case with the vocals in popular music. This idea can be extended to more than two channels, in which case multichannel diversity is often called \textit{spatial information}.
As a large number of recording nowadays is still stored as single channel, in this thesis I want to focus on this single channel separation only.

\subsubsection{Blind vs. Supervised Separation}
A blind separation source separation system does not require any additional information about the source signals or mixing system, the location or acoustical environment to perform separation~\cite{makino07}.
In practice it is known that the general blind source separation problem is ill-posed and it is not generally possible to find a solution.
This why many proposed methods rely on additional information such as in~\cite{liutkus13, ewert14}.
In this thesis I will study and present methods for both, blind and the supervised case.

\subsubsection{Vocal Accompaniment Separation}
Vocal Accompaniment separation has specific issues and assumptions when compared to other separation scenarios like speech.
Many music separation methods often rely on knowledge about the mixing process as made in Table~\ref{tab:mixing_models}.
While there exist many source separation methods that aim to extract the actual raw audio recording (Step 1 in  Table~\ref{tab:mixing_models}), often it is sufficient to extract the source images from the raw mixture.
In live recording this would result inverting the process of convolution as well.
In fact, separation of convoluted mixtures is very active field in source separation described in~\cite{pedersen07}.
In the context of music a separation, however, this becomes less relevant as today's recording and studio mixing environment is mostly digital.
This means that the last step in creating music mixtures, as described earlier, results in a linear mixture.
While the source images can yield from a mixing process undergoing the various assumptions, for the case of a mixture of source images, we consider usually only consider linear mixing in this thesis.
For a more detailed description of this scenario and applications of source image extraction, see~\cite{sturmel12}.\\

Another specific about music separation is that applications typical are restricted to a well defined set of musical sources.
These restrictions are typically made because not for all kind of music separation scenarios, data sets are available, which would make evaluation as well as training machine learning models difficult.
In music separation, by far the most popular task is to extract the vocals and the background of the music.
This allows for example automatic karaoke recordings.
An extensive overview of music separation methods can be found in~\cite{rafii18}.
Even though the overview is focused on vocal accompaniment separation, most approaches can be generalized to other sources.

\subsection{Source Count Estimation}
% copy from icassp paper maybe
Even the number of sources is a simple but very important information for source separation algorithms. One of the main drawbacks of many source separation systems is that they rely on this information. In some scenarios, like popular western music, the sources to separate are grouped into Melody + Bass + Drums and a residual signal. Constraining the system to such a scenario allows the results to be evaluated even if the set of sources being separated is incomplete. Constrained systems like these are also sufficient for real-world applications such as the eminent karaoke scenario. Limiting the number of desired sources helps not only to improve the performance of the algorithms but is also related to the fact that the number of sources humans can perceive is limited, too. Although a threshold has not been systematically addressed so far, a variety of experiments have been carried out. David Huron found \cite{huron89} that the number of voices humans can correctly identify is up to three.
% copied from my icassp 2018 paper
In a “cocktail-party” scenario with many concurrent speakers, a typical assumption is that the number of concurrent speakers is known.
In practice, almost all system assume the number of sources to be known even when they are aiming for a blind separation system.
Unfortunately, in real world applications, information about the actual number of concurrent speakers
is often not available.
Surprisingly, very few methods have been proposed to address the task of
counting the number of speakers.
%from source counting paper
For music signals, this task is especially challenging because the definition of a musical source can not clearly be given.

