\hypertarget{fundamentals}{%
\chapter{Fundamentals}\label{fundamentals}}

The main focus of this thesis is the \emph{analysis} and \emph{processing} of sound recordings of music or speech, commonly referred to as \emph{audio signals}.
In this chapter, I introduce basic concepts of digital audio signals which are relevant to follow the remaining chapters.

\hypertarget{Fundamentals of Overlapped Sounds}{%
\section{Fundamentals of Overlapped Sounds}\label{specifics-of-audio-signals}}

When a sound wave travels through a medium like air it can be captured using a microphone by measuring the local pressure deviation over time.
Such a signal ca be written as a function \(x(t)\) continuous in both, time \(t \in \sR\) and the amplitude \(x(t) \in \sR\).
An \emph{audio signal} can simply be introduced as a signal that is meant to be perceived by the human auditory system --- through our ears.
Therefore, one could observe specifics properties of audio signals, consistent to the limitations of the human hearing system, for example in dynamics/loudness as well as a limited signal bandwidth.
In fact, many other signals exist with similar characteristics such as signals from finance, geophysics, meteorology or medical data.
The result is that often audio research is inspired by applications of other fields of signal processing and vice versa.

\hypertarget{digital-representations-of-audio-signals}{%
\subsection{Digital Representations of Audio
Signals}\label{digital-representations-of-audio-signals}}

To store, analyze or process audio signals, as of today, digital representations are used to leverage the processing capabilities of modern computing devices.
A digital audio signal can be obtained from an analog signal using analog-to-digital/digital-to-analog converters (ADC/DAC) which can be found in almost any every-day device such as laptops and smart-phones.
In short, this includes to steps, First, the continuous time signal \(x(t)\) is converted to a discrete time series, where one sample\footnote{Please note, that the use of the word \emph{sample} will have different meanings in the context of machine learning, where a sample is usually a instance of a signal instead of a single time step.} \(x(n)\) using equidistant steps \(T\) using \emph{sampling} and Second, the amplitude values are \emph{quantized} resulting in vector \(\vx \in \sZ\), represented as a one dimensional time series of amplitudes.
An important parameter in the process of digitization is the sample rate \(F_s = 1/T\) where \(T\) is the sampling period.
Often, \(F_s\) has a significant effect on the quality of audio signals.
And, it is the objective of a real world audio system to not introduce perceptible degradation in audio quality when a digital signal is reproduced over headphones or loudspeakers.
Due to the Nyquist-Shannon sampling theorem the sample rate needs to be at least twice the band width of the analogue signal.
Since the hearing range of normal listening humans is
20~\si{\hertz} - 20~\si{\kilo\hertz}~\cite{fastl90, moore89}, typically for audio signals, 
sample rates of 44100~\si{\hertz} are chosen to facilitate the full human hearing range.
However, for many applications, a lower sampling rate is sufficient, e.g. in speech communication where intelligibility often is more important than quality.
For further details we refer the reader to audio signal processing basics such as~Chapter 1 in~\cite{proakis96} or Chapter 2 in~\cite{Mueller15}.

\hypertarget{time-frequency-representation}{%
\subsection{Time-Frequency Representation}\label{time-frequency-representation}}

Real world sounds such as speech and music include periodicity.
Therefore, sounds are often analyzed in the frequency domain as a \emph{spectrum} to reduce redundancy and improve the computational efficiency of the signal processing.
This is commonly achieved trough the use of discrete Fourier transform (DFT). 
For details, the reader is referred to~Chapter 4 of~\cite{proakis96}.
At the same time, spectral representations relates to our human auditory system~\cite{zwicker13, moore89, bregman90} allowing us to process sounds the way we perceive them.
\par
The periodicity of real world sounds, as mentioned above, usually only holds for short time scales of several milliseconds, often referred to as ``quasi-stationarity''.
Instead, sounds are non-stationary over the course of several seconds and it therefore is useful to analyze and process short-time spectra, computed in an overlapped fashion, resulting in a \emph{time-frequency} (TF) representation.
Simply put, a TF representation encodes the time-varying \textit{spectra} into a matrix \(\mX(k\) with frequencies \(k\) and time frames \(n\).
For signals with more than one channel, the representation is extended to yield a three-dimensional tensor, where \(c\) includes the channel.
\par
When sounds are processed in the time-frequency domain the transformation needs to be inverted, hence many researchers chose a representation that allows to easily reconstruct the signal.
The short-time Fourier transform (STFT) is the most commonly used TF representation~\cite{mcaulay86}.
STFT matrices \(\mX \in \sC^{n \times k \times c}\) are complex meaning that they include the phase information along with and the magnitude which represents the amplitude of that signal.
Often the magnitude of the STFT is referred to as the~\emph{spectrogram}.

\subsection{Fundamental Frequency, Pitch and Harmonicity}
% TODO: Add STFT plot from Cello Dataset, showing f0, overtones vibrato extend etc.
% \cite{stoeter15acm}, to be downloaded in \cite{oss}

% from zafar
Speech and music signals contain is characterized by its periodicity.
And it is this property that humans perceive as \emph{pitched}.
\emph{Pitch} is defined by Klapuri~\cite{klapuri06book} as ``a perceptual attribute which allows the ordering of sounds on a frequency-related scale extending from high to low''.
It is important to note that the term \emph{pitch} is a subjective measure.
The objective equivalent is referred to as the fundamental frequency (\(F_0\))\footnote{Pitch and $F_{0}$ are often used synonymously in audio research. Even though this is incorrect I will sometimes refer other work where pitch is instead of $F_{0}$.}.
\(F_{0}\) is can be defined as the lowest frequency/partial of an harmonic signal.
All frequencies together formed by the integer multiples of the fundamental frequency are named  \textit{harmonics}~\cite{schenker54}.
When the fundamental frequency changes, the frequencies of these harmonics changes accordingly.
This results in the typical comb-like spectrograms of harmonic signals.
For a a detailed overview into the research field of pitch and \(F_{0}\), the reader is referred to~\cite{decheveigne05, klapuri06book, salamon13}.
\par
An estimate of the fundamental frequency of a signal is required in various applications of audio and speech signal processing.
Some scenarios are targeted to extract the fundamental frequency of the predominant source~\cite{salamon12} in a mixture of other sources.
In other applications, algorithms are used to extract fundamental frequencies of multiple sources simultaneously present in a signal~\cite{klapuri03}.
However, the most common scenario in many works is to extract the fundamental frequency of a monophonic and harmonic audio signal containing speech or music~\cite{talkin95, boersma02, decheveigne02, resch07, tidhar10, christensen07, stoeter15icassp}.

\subsection{Modulations}

\subsubsection{Amplitude Modulation}

Amplitude modulation (AM) describes the modulation of a carrier signal, for example \(x(t) = \cos \omega_c t\) by a modulation function \(a(t)\):

\begin{align}
    s_{AM}(t) &= a(t) \cos \left( \omega_{c} t\right)
\end{align}

It is assumed that the modulation function is periodic and of slower frequency than the carrier signal.
AM is very widely used in many signal processing applications like radio transmission.


\subsubsection{Frequency Modulation}
\begin{align}
    s_{FM}(t) = A \cos(\phi(t))
\end{align}

Frequency  modulation  caused  by  vibrato  is  a  very  common
playing  style  for  string  instruments  but  also  for  woodwind  and
brass instruments.


\subsubsection{Modulation Speed}

Vibrato (FM), Tremolo AM 

\hypertarget{sources-and-mixtures}{%
\section{Sources and Mixtures}\label{sources-and-mixtures}}

The definition of a sound source builds upon the underlying physical
phenomenon of the actual \emph{acoustical} emission of a sound and its
spatial position.
In the real world, however, single isolated sound sources are rare.
Instead we are often faced with multiple sources that render a called acoustical sound scene.
In this setting, one can define a \emph{set of sources} where the set is of arbitrary size.
Sets can include other sets, thus representing acoustical scenes of hierarchical structure.

When multiple sources are active at the same time the sound that reaches our ears or is recorded using a microphone is superimposed or \emph{mixed} into a single sound.
The \emph{mixture} is a mapping from a set of sources
\(\mathbf{s_j}\) to a target \(\mathbf{x}\).
There exist a variety of different mixing models that are utilized in literature and throughout this thesis.
Usually they are build upon several assumptions to constrain the scenario and model specific aspects of real world signals.
The most important assumption is that the mixing is linear so that the sum of all sources result to the mixture.
The non-linear case would be relevant for professionally produced music where audio effects are applied in post-processing.
While this might not represent all real world mixtures, it is usually a good approximation.
Another distinction is between instantaneous or convolutive mixtures.
For instantaneous mixtures, all sources are mixed using fixed mixing parameters \(a_j\).
This is the typical scenario when sources are mixed using a mixing console (pan-pot mix).
In \emph{convolutive} mixtures, each source \(\mathbf{s_j}\) is convolved with a filter response \(r_j\).
This scenario approximates a real world acoustic environment like a cocktail party where each speaker is then convoluted with a room impulse response.
Usually, the mixing process is assumed to be time-invariant but for a variety of signals such as EEG data or music live recording with moving sources it can also be time-variant.
In the remainder of this thesis, however, I will only consider the time-variant case.
I summarize the mathematical notation of different mixing models in Table~\ref{tab:mixing_models}.

\begin{table}[]
    \centering
\begin{longtable}[]{lll}
\toprule
\begin{minipage}[b]{0.26\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.42\columnwidth}\raggedright
Instantaneous\strut
\end{minipage} & \begin{minipage}[b]{0.23\columnwidth}\raggedright
Convolutive\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.26\columnwidth}\raggedright
Time-Invariant\strut
\end{minipage} & \begin{minipage}[t]{0.42\columnwidth}\raggedright
\(\mathbf{x}=\sum_{j=1}^{J}a_j\mathbf{s}_j\)\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
\(\mathbf{x} = \sum_{j=1}^{J}r_{j} \ast \mathbf{s}_j\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.26\columnwidth}\raggedright
Time-Variant\strut
\end{minipage} & \begin{minipage}[t]{0.42\columnwidth}\raggedright
\(\mathbf{x}=\sum_{j=1}^{J}a_j(n)\mathbf{s}_j\)\strut
\end{minipage} & \begin{minipage}[t]{0.23\columnwidth}\raggedright
\(\mathbf{x} = \sum_{j=1}^{J}r_{j}(n) \ast \mathbf{s}_j\)\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}
    \caption{Overview of linear mixing models.}
    \label{tab:mixing_models}
\end{table}

% from zafar >> rewrite
The mixing process can equivalently be described in the TF domains as the {DFT} is a linear operator.
Therefore the summation of two signals in time domain extends to the frequency domain.
However, since we are normally dealing with the non-negative magnitude representation this is only an approximation~\cite{klapuri06}.

\paragraph{Specifics of Music Mixtures}
\label{par:specifics_of_music_mixtures}

In music it is important to understand that the process of mixing is of paramount importance to successfully model music.
This is because mixing sources is often considered as a creative task that involves recording engineers and tonmeisters.
In today's recording processes which involve digital mastering, professionally produced music consists of several intermediate mixing steps before the final mixture is produced:

\begin{description}
  \item[1) Microphone Recording:] in this step the analog sources are captured and D/A converted. Vocals and other acoustic instruments are recorded using one or multiple microphones.
  Electric instruments such as keyboards or synthesizers may be amplified and then directly digitized.
  \item[2) Raw Source Image:] the digital raw source signals are then grouped and mixed together into what is called a \emph{source image\footnote{Sometimes this is referred to as ``stem''.}}.
  This grouping is involves a creative process, hence it is usually done by a recording engineer.
  The source image is usually mixed to a specific number of output channels (often: stereo) even though the recording is done by a mono microphone (e.g. vocals) or multiple microphones (e.g. drums).
  In this stage a linear panning is added to spatially position either the sources or the image.
  \item[3) Mastered Source Image:] for each of the images an additional mastering step could be applied that may involves additional processing.
  At this stage, often artificial reverberation is added.
  \item[4) Raw Mix:] all source images are mixed for the intended target such as a stereo.
  This step is usually a linear sum of all images.
  \item[5: Mastered Mix:] again, an additional mastering is applied.
  Often this step involves non-linear processing such as dynamic range compression.
\end{description}

In each of these steps, one of the mixing models described in Table~\ref{tab:mixing_models} can be assumed which is why modeling music mixtures is considered to be a very challenging problem.
In fact, this emphasizes that the definition of a musical audio source cannot clearly be given as it is subjective and depends on the application and its context.
This is problematic for various applications that deal with modeling mixtures and their underlying sources.

\hypertarget{processing-and-analysis-of-mixtures}{%
\section{Processing and Analysis of Mixtures}\label{processing-and-analysis-of-mixtures}}

Speech is one of the most important signal types as it is fundamental for humans to enable communication.
When humans talk to each other, we are often not aware of the fact that we are listening to a mixture of several sources (e.g. from multiple talkers) or noise.
For many applications, some of the sources in these mixtures may not be desired~\cite{lorem}.
Often a specific talker is considered as the desired source used to carry the actual information whereas the noise or other speakers interfere with this signal.
The attenuation of undesired speakers when multiple concurrent speakers are active is well known as the ``cocktail party problem''~\cite{cherry53, haykin05}.
Surprisingly, it has been shown that humans are able to carry out this task for a desired source, even without eye contact and when not spatial cues can be used~\cite{bregman90}.
Therefore, for a long time, research is fascinated by the idea to create a machine to inverse the process of mixing  and to extract or separate the desired sources from its mixture.
This problem is called \emph{source separation}.
% It is similar to when we make a photograph of an object with many other objects visible in the same scene, sometimes occluding the object of interest.\\

\subsection{Source Separation}
The earliest work on audio source separation started in the mid 50s~~\cite{} on speech data and remains today is still a very active field of research with a large number of contributions.
It yields very specific challenges and opportunities and in the past a huge number of scientific contributions were made.
Source separation methods has both relevant applications for music and speech mixtures such as X, Y, Z. Often it enables other tasks such as A, B, C, D.
Due to both its importance and their large number of applications, source separation has been a popular topic in signal processing for decades.\\
% from zafar
While early works started with speech separation, the separation of musical sources is dating back to the 1970's.
They are mostly based on classic digital signal processing techniques, generally attempt to exploit the pitch information of the lead musical source and typically require some manual interaction~\cite{miller73, oppenheim68, oppenheim682}.
Due to the high number of contributions in this research field, it is not feasible to give an extensive overview on existing methods in the context of this thesis.
Often researchers do not deal with the general source separation problem but propose contributions to specific sub problems, usually targeted at a much more constraint scenario.
I therefore decided to present my key decisions that I made for the work in this thesis and I refer the readers to overview literature with respect to other scenarios when appropriate.

\subsubsection{Underdetermined vs. Overdetermined Separation}
As mentioned in Section~\ref{sources-and-mixtures}, generating sound mixtures is closely related to the process of the mixing taken place during recording (speech) or with the help of professional recording engineers (music).
One assumption that was not mentioned before, is the importance of the number sensors or microphones used to create the mixture.
A source separation problem is \emph{over-determined} when the number of sources is smaller than the number of sensors; \emph{determined} when they are equal.
For these two cases, a large number of method exist and in same cases a closed form solution is possible.
The reader is referred to~\cite{common10}, which is gives a detailed overview of these methods.\\
Many real world source separation problems, however, are under-determined and up to date for a large number of scenarios, the problem of separating sources is still very challenging.
In this thesis I only focus on methods that perform separation on underdetermined mixtures.
% Todo: add argumentation here

\subsubsection{Single Channel vs. Multichannel Separation}
% from zafar
Approaches based on panning information aim at exploiting stereo cues to identify and separate individual sources. Such approaches typically compare the left and right channels of a mixture in the TF domain to estimate the \textit{panning coefficients} of the sources and then generate TF masks to separate them. They generally assume that the one source has a fixed panning, which is often the case with the vocals in popular music. This idea can be extended to more than two channels, in which case multichannel diversity is often called \textit{spatial information}.
As a large number of recording nowadays is still stored as single channel, in this thesis I want to focus on this single channel separation only.

\subsubsection{Blind vs. Supervised Separation}
A blind separation source separation system does not require any additional information about the source signals or mixing system, the location or acoustical environment to perform separation~\cite{makino07}.
In practice it is known that the general blind source separation problem is ill-posed and it is not generally possible to find a solution.
This why many proposed methods rely on additional information such as in~\cite{liutkus13, ewert14}.
In this thesis I will study and present methods for both, blind and the supervised case.

\subsubsection{Vocal Accompaniment Separation}
Vocal Accompaniment separation has specific issues and assumptions when compared to other separation scenarios like speech.
Many music separation methods often rely on knowledge about the mixing process as made in Table~\ref{tab:mixing_models}.
While there exist many source separation methods that aim to extract the actual raw audio recording (Step 1 in  Table~\ref{tab:mixing_models}), often it is sufficient to extract the source images from the raw mixture.
In live recording this would result inverting the process of convolution as well.
In fact, separation of convoluted mixtures is very active field in source separation described in~\cite{pedersen07}.
In the context of music a separation, however, this becomes less relevant as today's recording and studio mixing environment is mostly digital.
This means that the last step in creating music mixtures, as described earlier, results in a linear mixture.
While the source images can yield from a mixing process undergoing the various assumptions, for the case of a mixture of source images, we consider usually only consider linear mixing in this thesis.
For a more detailed description of this scenario and applications of source image extraction, see~\cite{sturmel12}.\\

Another specific about music separation is that applications typical are restricted to a well defined set of musical sources.
These restrictions are typically made because not for all kind of music separation scenarios, data sets are available, which would make evaluation as well as training machine learning models difficult.
In music separation, by far the most popular task is to extract the vocals and the background of the music.
This allows for example automatic karaoke recordings.
An extensive overview of music separation methods can be found in~\cite{rafii18}.
Even though the overview is focused on vocal accompaniment separation, most approaches can be generalized to other sources.

\subsection{Source Count Estimation}
% copy from icassp paper maybe
Even the number of sources is a simple but very important information for source separation algorithms. One of the main drawbacks of many source separation systems is that they rely on this information. In some scenarios, like popular western music, the sources to separate are grouped into Melody + Bass + Drums and a residual signal. Constraining the system to such a scenario allows the results to be evaluated even if the set of sources being separated is incomplete. Constrained systems like these are also sufficient for real-world applications such as the eminent karaoke scenario. Limiting the number of desired sources helps not only to improve the performance of the algorithms but is also related to the fact that the number of sources humans can perceive is limited, too. Although a threshold has not been systematically addressed so far, a variety of experiments have been carried out. David Huron found \cite{huron89} that the number of voices humans can correctly identify is up to three.
% copied from my icassp 2018 paper
In a “cocktail-party” scenario with many concurrent speakers, a typical assumption is that the number of concurrent speakers is known.
In practice, almost all system assume the number of sources to be known even when they are aiming for a blind separation system.
Unfortunately, in real world applications, information about the actual number of concurrent speakers
is often not available.
Surprisingly, very few methods have been proposed to address the task of
counting the number of speakers.
%from source counting paper
For music signals, this task is especially challenging because the definition of a musical source can not clearly be given.
