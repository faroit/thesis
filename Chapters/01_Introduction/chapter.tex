\chapter{Introduction}\label{ch:introduction}

It is very likely that you know the following situation: you were at a crowded party and the next day your best friend, who was unable to join, asked you ``How many people were there?''.
You then struggled to find an answer because you had such intense conversations that you were unable to put your focus on the other guests.
This scene includes many interesting aspects that are relevant to this thesis. Notably, it reminds us that in our daily life we are exposed to situations where multiple events overlap in time.
\par
In contrast to our vision, although we might be physically able to \emph{hear} all sounds, we deliberately choose to \emph{listen to} a few sources and attenuate others.
In a noisy environment, we can steer our attention to one sound source, even without eye contact and using only a single ear~\cite{bregman90}.
However, this attention mechanism prevents us from observing the acoustic scene as a whole.
This ability to concentrate on a single source is not limited to conversations --- it also applies to music.
If we imagine attending a music concert, it is likely that we focus on the lead vocalist and miss out many details of the background band, demonstrating our ability to \emph{separate} audio mixtures, at least cognitively.
\par
In the audio research community, the task of attenuating undesired speakers when multiple concurrent speakers are present is known as the ``cocktail party problem''~\cite{haykin05}.
For the past 70 years~\cite{cherry53}, researchers have been fascinated by this idea to computationally imitate this ability of humans to separate the sources in a mixture.
In the general setting, which is not restricted to the cocktail party scenario but also includes music processing, this problem is called \emph{source separation} and is one of the key topics considered in this thesis.
\par
Although most scientific efforts have focused on separation, our example highlights the fact that even the number of sources is a valuable information.
From a more technical point of view, many separation methods rely on prior knowledge of the number of sources, requiring this information to be estimated beforehand, or provided by a user~\cite{liutkus13}.
As we illustrated, humans are not very good at estimating the number of sources based on audio which sharply contrasts with our ability to focus on a single source in a crowded audio scene.
\par
Both scenarios of music and speech mixtures have in common that estimating the number of sources and separating them becomes more challenging when the signals are more \emph{overlapped}.
And both tasks become even more challenging when sources are almost entirely overlapped such as when multiple instruments are playing the same note (in unison).
In this unique scenario where sources are overlapped in time and frequency,
observable differences between sources are difficult to obtain at a short time scale.
However, differences do appear when we consider longer time contexts, for which the variations of sources over time become important.
For instance, speech and the sound of musical instruments can have distinct modulations such as vibrato, created by conscious physical manipulation of the sound, to make a sound more intelligible or pleasant.
\par
In this thesis, we aim to investigate if modulations can be utilized for analyzing and processing of highly overlapped speech and music source signals.
For this, we first study and develop new representations that could improve the analysis and processing of such signals, to address if modulations are automatically detected or extracted from highly overlapped signals.
Second, we develop new methods built upon such representations to address source separation. 
These methods are designed for constrained scenarios where modulation effects can easily be exploited.
Third, to investigate and develop new methods to address the task of estimating the number of sources in highly overlapped mixtures.
Finally, we want to show how such research may be transferred from synthetic signals to real-world scenarios.

\section{Summary of Contributions}

This thesis contains five main contributions:

\begin{enumerate}
\item I reviewed scenarios of time and frequency overlapped audio sources.
I studied a scenario where instruments are highly overlapped (unison) but I also considered known scenarios for speech and music.
In this unison scenario, I reviewed how slowly-varying tempo-spectral modulations, caused e.g. by vibrato, can be utilized for separation and source count estimation of highly overlapped signals.
Furthermore, I showed how these scenarios can stimulate new research directions to \emph{analyze} and \emph{process} such signals.

\item I designed two novel methods to \emph{separate} unison instrument mixtures: one is informed by an estimate of the fundamental frequency variation.
The other is unsupervised, inspired by the way how humans segregate time-varying sources.
Along the way, I also proposed a post-processing to improve \(F_0\) estimates based on the same principles.
Next, I studied how the observations from the unison scenario can be transferred to real-world scenarios such as lead accompaniment separation by applying the deep learning framework.

\item I conducted two detailed experimental studies to assess how humans perceive highly overlapped mixtures and how they perform when asked to estimate the number of sources.
In these studies, I focussed on scenarios of overlapped speech as well as polyphonic music recordings.
Both studies confirmed previous work, indicating that humans can only correctly estimate the number of concurrent sources up to three.

\item We designed a method to automatically estimate the maximum number of concurrent speakers. This method uses deep neural networks to addresses ``cocktail party'' like environments.
This model reached state-of-the-art performance when compared to other models and also supersedes human performance when compared with the results of my subjective listening experiments.
Finally, I revealed the relation between slow modulations in speech and the ability of a model of \emph{learning to count}.

\item As a practical contribution, I developed tools to assess the quality of the separation system using interactive web applications. 
I helped to create publicly available datasets for separation and \(F_0\) estimation.
Furthermore, I co-organized the \ac{SiSEC} to improve sustainability and reproducibility for the research community.
\end{enumerate}

\clearpage

\section{Structure of this Thesis}

The thesis and its relevant linked publications are organized into six main chapters.
\begin{description}
  \item[Chapter~\ref{cha:fundamentals}] explains the fundamental concepts of audio signals (Section~\ref{sec:specifics-of-audio-signals}), sources and overlapped sounds (Section~\ref{sources-and-mixtures}), relevant for the remainder of this thesis.
  Furthermore, the process of mixing sound sources as well as its inverse task --- sound source separation --- are explained.
  The chapter also covers basics of fundamental frequency and its variations (Section~\ref{sub:time-variant-audio-signals}) as an important feature for harmonic audio signals.
  \item[Chapter~\ref{cha:highly-overlapped-signals}] introduces relevant tasks and applications in the context of highly overlapped sounds.
  Furthermore, the importance of slow modulations in this context is discussed (Section~\ref{exploiting-slow-modulations}).
  \item[Chapter~\ref{cha:datasets}] presents and discusses the importance of data for analysis and evaluation.
  In this chapter we present a dataset for unison instrument mixtures (Section~\ref{sec:unison_dataset}~\cite{oss_unison, stoeter14}) and a dataset for precise fundamental frequency estimates (Section~\ref{sec:muserc}~\cite{oss_muserc, stoeter15acm}). Furthermore we also present a short overview of relevant multitrack datasets for music separation~\cite{liutkus17, stoeter18sisec}.
  \item[Chapter~\ref{cha:known}] presents separation methods that are developed in this thesis. This covers techniques that utilize modulation information, when available. We present a method based on time warping using $F_0$ estimates for unison mixtures (Section~\ref{sub:frequency_modulation}~\cite{stoeter14}) and show how it can be extended for the scenario of vocal and accompaniment separation (Section~\ref{sec:extendingf0}). Furthermore, we present a method to improve the precision of $F_0$ estimates (Section~\ref{sec:f0method}~\cite{stoeter15icassp}).
  \item[Chapter~\ref{cha:unknown}] presents separation methods utilizing modulation when prior information is not available. We present the \textsc{Common Fate Model} based on tensor factorization for unison mixtures (Section~\ref{sec:the_common_fate_model_for_unison_mixtures}~\cite{stoeter16}) and also propose an extension based on \acs{DNN} for vocal and accompaniment separation (Section~\ref{sec:cft_for_lead_accompaniment_separation}).
  \item[Chapters~\ref{cha:countanalysis}] presents our listening experiments to find out what the number of sources is that humans are able to identify in music~(Section~\ref{sec:ismir}~\cite{schoeffler13, stoeter13}) or concurrent speech~(Section~\ref{sec:count_experiment}~\cite{stoeter19, stoeter18}). 
  \item[Chapters~\ref{cha:countnet}] presents \textsc{CountNet}, our method to address the source count estimation problem using a data-driven model in a simulated ``cocktail-party'' scenario~\cite{stoeter19}.
  \item[Chapter~\ref{cha:conclusion}] concludes this thesis and gives an outlook into future research directions.
\end{description}
